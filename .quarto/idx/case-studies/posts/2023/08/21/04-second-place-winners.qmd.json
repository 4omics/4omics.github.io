{"title":"Food for Thought: Second place winners -- DeepFFTLink","markdown":{"yaml":{"title":"Food for Thought: Second place winners -- DeepFFTLink","description":"PhD students and faculty from Worcester Polytechnic Institute and Indiana University Bloomington describe their solution to the Food for Thought challenge: an ensemble of base string matching and BERT models.\n","categories":["Machine learning","Natural language processing","Public policy","Health and wellbeing"],"author":"Yang Wu, Aishwarya Budhkar, Kai Zhang, Xuhong Zhang, and Xiaozhong Liu","date":"08/21/2023","toc":true,"bibliography":"references.bib","image":"images/04-deepfftlink.png","image-alt":"A shopper walks the aisles of a supermarket in the United States."},"headingText":"Perspective on the challenge","containsRefs":false,"markdown":"\nDeepFFTLink team members: Yang Wu and Kai Zhang are PhD students at Worcester Polytechnic Institute. Aishwarya Budhkar is a PhD student at Indiana University Bloomington. Xuhong Zhang is an assistant professor at Indiana University Bloomington. Xiaozhong Liu is an associate professor at Worcester Polytechnic Institute.\n\nText matching is an essential task in natural language processing [NLP, @DBLP:journals/corr/PangLGXWC16], while record linkage across different sources is an essential task in data science. Machine learning techniques allow people to combine data faster and cheaper than using manual linkage. However, in the context of the Food for Thought challenge, existing methods for matching universal product codes (UPCs) to ensemble codes (ECs) require every UPC to be compared with every EC code (Figure 1a). Such approaches can be computationally expensive in the training process when data is noisy. Here, we propose an ensemble model with a category-based adapter to tackle this problem, drawing on the category information included in UPC and EC data. The category-based adapter allows UPCs to be first matched with only a small and reliable set of ECs (Figure 1b). Then, an ensemble model will be deployed to make predictions for UPC-EC matching. Our proposed approach can achieve competitive performance compared with state-of-the-art models.\n\n:::{layout-ncol=2 style=\"padding-top: 1em; margin-bottom: 0;\"}\n[![(a)](images/pt4-fig1a.png)](images/pt4-fig1a.png)\n\n[![(b)](images/pt4-fig1b.png)](images/pt4-fig1b.png)\n:::\n\n:::{.figure-caption}\n**Figure 1:** A toy example of our method. Panel (a) shows the traditional matching method, while (b) is our proposed ensemble model with category-based adapter. With the help of the adapter, UPC 1 only needs to be matched with EC 1 and EC 3.\n:::\n\n## Our approach \nWe propose a two-step framework to address this problem. To begin with, we use a category-based adapter to get reliable candidate ECs for each UPC. Then, an ensemble model [@10.1007/3-540-45014-9_1] is deployed to make a prediction for each UPC-EC pair. \n\n##### Category-based adapter \nBy using 2015--2016 UPC-EC data, we created a knowledge base, which is a UPC category--EC pair-wised table for generating candidate ECs. Within this setting, each UPC category is, on average, related to only 32 ECs. This knowledge base is then used as context to further filter the candidate ECs. Note that there are some new ECs generated year by year, which can also be part of the potential ECs in the UPC-EC matching task, since the contextual information of new ECs does not exist in our knowledge base. \n\n##### Ensembled model\nWe ensemble the base-string match and BERT models. BERT is a deep learning model for natural language processing [@DBLP:journals/corr/abs-1810-04805]. In the base-string match model, we used the Term Frequency-Inverse Document Frequency (TFIDF) of each UPC and EC description as features to calculate a pairwise cosine similarity, which is a distance between instances. Meanwhile, we used features extracted from UPC and EC descriptions to fine-tune the BERT base model and calculated the cosine similarity of embeddings between each UPC and EC. Then we rank ECs based on their similarity scores with the UPC.\n\n[![](images/pt4-fig2.png)](images/pt4-fig2.png)\n\n:::{.figure-caption}\n**Figure 2:** The framework of our proposed model. A two-step strategy is used to make the final prediction.\n:::\n\n::: {.callout-note style=\"margin-top: 2rem;\" appearance=\"simple\"}\nFind the code in the [Real World Data Science GitHub repository](https://github.com/realworlddatascience/realworlddatascience.github.io/tree/main/case-studies/posts/2023/08/21/_code).\n:::\n\n## Our results \nWe randomly selected 500 samples from the 2017--2018 UPC-EC data to train the ensembled weight for each model. Two functions were adapted to make a fusion of base-string and BERT models:  \n\n$$\nC = a * X + b * Y  \n$$ {#eq-first}\n\n$$\nC =  a * log(X) + b * log(Y) \\text{. }\n$$ {#eq-second}\n\n$C$ denotes the final confidence score. $X$ and $Y$ represent *base_string_similarity_score* and *BERT_similarity_score*, respectively. $a$ and $b$ are corresponding model weights for base_string and BERT models.\n\nA better Success@5 is achieved with function (1). The ensembled weights for the base-string model and BERT model are 0.738 and 0.262, respectively. The experiment result indicates that the base_string model contributes more than the BERT model when the ensemble model makes predictions. The prediction result for the 2017–2018 data is:\n \n- Success@5: 0.727 \n- NDCG@5: 0.528  \n\nComputation time is 6 hours. \n\n## Future work \nOur next step will focus on adding the newly generated EC data into our knowledge base, which allows the model to be more stable to make predictions for UPC-EC matching. Our model is an unsupervised method, which does not need labels for each instance. We use cosine similarity to rank the matches, so no labels are needed in the training process. However, our future work will try to label some instances to handle the UPC-EC matching task in a supervised manner. \n\n## Lessons learned \n1. **If the data is not complex, simple models may outperform complex models.** For example, in our experiment, we found that the base-string model outperforms single RoBERTa [@DBLP:journals/corr/abs-1907-11692] or BERT models. However, our ensemble model can outperform each individual model since model fusion allows information aggregation from multiple models. \n\n2. **Multi-label models may not work well on UPC-EC data.** In our early work, we tried to consider the UPC-EC matching task as a multi-label problem, e.g., we labeled each EC as a binary label which indicated whether the EC was an appropriate match or not. We mapped UPC and EC pairs into a multi-label table. However, we find that the UPC and EC keeps a one-to-one relation for most UPCs. The model performance of a multi-label model, i.e., Label-Specific Attention Network [LSAN, @xiao-etal-2019-label], is lower than base-string model on both Success@5 and NDCG@5 metrics. \n\n::: nav-btn-container\n::: grid\n::: {.g-col-12 .g-col-sm-6}\n::: {.nav-btn}\n[&larr; Part 3: First place winners](/case-studies/posts/2023/08/21/03-first-place-winners.qmd)\n:::\n:::\n\n::: {.g-col-12 .g-col-sm-6}\n::: {.nav-btn}\n[Part 5: Third place winners &rarr;](/case-studies/posts/2023/08/21/05-third-place-winners.qmd)\n:::\n:::\n:::\n:::\n\n::: further-info\n::: grid\n::: {.g-col-12 .g-col-md-12}\nAbout the authors\n: **Yang Wu** and **Kai Zhang** are PhD students, and **Xiaozhong Liu** is an associate professor at Worcester Polytechnic Institute. **Aishwarya Budhkar** is a PhD student and **Xuhong Zhang** is an assistant professor at Indiana University Bloomington.\n:::\n::: {.g-col-12 .g-col-md-6}\nCopyright and licence\n: © 2023 Yang Wu, Aishwarya Budhkar, Kai Zhang, Xuhong Zhang, and Xiaozhong Liu\n\n<a href=\"http://creativecommons.org/licenses/by/4.0/?ref=chooser-v1\" target=\"_blank\" rel=\"license noopener noreferrer\" style=\"display:inline-block;\"> <img src=\"https://mirrors.creativecommons.org/presskit/icons/cc.svg?ref=chooser-v1\" style=\"height:22px!important;vertical-align:text-bottom;\"/><img src=\"https://mirrors.creativecommons.org/presskit/icons/by.svg?ref=chooser-v1\" style=\"height:22px!important;margin-left:3px;vertical-align:text-bottom;\"/></a> This article is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0) <a href=\"http://creativecommons.org/licenses/by/4.0/?ref=chooser-v1\" target=\"_blank\" rel=\"license noopener noreferrer\" style=\"display:inline-block;\"> International licence</a>. Thumbnail photo by <a href=\"https://unsplash.com/@hansonluu?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText\">Hanson Lu</a> on <a href=\"https://unsplash.com/photos/sq5P00L7lXc?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText\">Unsplash</a>.\n  \n\n:::\n::: {.g-col-12 .g-col-md-6}\nHow to cite\n: Wu, Yang, Aishwarya Budhkar, Kai Zhang, Xuhong Zhang, and Xiaozhong Liu. 2023. \"Food for Thought: Second place winners -- DeepFFTLink.\" Real World Data Science, August 21, 2023. [URL](https://realworlddatascience.net/viewpoints/case-studies/posts/2023/08/21/04-second-place-winners.html)\n:::\n:::\n:::"},"formats":{"html":{"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":"auto","echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"markdown"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../../../../../styles.css"],"toc":true,"output-file":"04-second-place-winners.html"},"language":{},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.2.335","theme":["lux","../../../../../styles.css"],"page-layout":"article","grid":{"sidebar-width":"0px","body-width":"3000px","margin-width":"250px"},"title-block-banner":true,"sidebar":false,"comments":{"giscus":{"repo":"realworlddatascience/realworlddatascience.github.io","category":"General"}},"title":"Food for Thought: Second place winners -- DeepFFTLink","description":"PhD students and faculty from Worcester Polytechnic Institute and Indiana University Bloomington describe their solution to the Food for Thought challenge: an ensemble of base string matching and BERT models.\n","categories":["Machine learning","Natural language processing","Public policy","Health and wellbeing"],"author":"Yang Wu, Aishwarya Budhkar, Kai Zhang, Xuhong Zhang, and Xiaozhong Liu","date":"08/21/2023","bibliography":["references.bib"],"image":"images/04-deepfftlink.png","image-alt":"A shopper walks the aisles of a supermarket in the United States."},"extensions":{"book":{"multiFile":true}}}}}