[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "4omics",
    "section": "",
    "text": "We are a bioinformatics company based in Greece, specializing in processing and analyzing high-throughput sequencing data. With a team of interdisciplinary experts, we deliver customized bioinformatics solutions to meet the unique needs of our clients.\nWe are here to partner with you! We are here to collaborate with you, providing tailored and personalized bioinformatics services that meet your specific needs. Together, we will unlock the full potential of your data and drive impactful discoveries.\n\n\n\n\nOur specialized bioinformatics services empower you to achieve higher standards and impactful outcomes.\n\n\n\nas we are guided by a team of experienced and dependable professional bioinformaticians and researchers.\n\n\n\n\n\n\nConsultation on commonly used bioinformatics analysis approaches\nWorkflow design, implementation, and result interpretation\nUtilization of Machine Learning methods on available omics and metadata and data integration\nExperimental design review\nGrant review (methods, data management plans, outreach)\nScientific data visualization\nSoftware as a service. Development of specialized tools (e.g., visualization) based on client needs.\nAssessment of research compliance with FAIR Principles (FAIR assessment/pipelines/data) - build repositories for your publications!!\nAdaptation of bioinformatics analyses based on the “Green Comput model"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Maria Tsagiopoulou\nCo-founder / CEO, Biologist / Bioinformatician\nDr. Maria Tsagiopoulou is a postdoctoral researcher at the National Centre for Genomic Analysis in Spain, one of Europe’s largest Genome Centres. She has more than 9 years of experience in analyzing and interpreting omics data to identify patterns in the genome, transcriptome, and epigenome levels that correlate with the pathogenesis and the evolution of diseases. Her research includes aspects of multiple disciplines, with a particular focus on immunology and oncology, utilizing bioinformatics and machine learning approaches to analyze omics data. Her expertise lies at the intersection of bioinformatics and deep knowledge of biology, enabling effective collaboration with researchers across various life science domains and clinicians. She has a comprehensive understanding of controlled vocabularies in biology, medicine, and computer sciences, facilitating efficient interdisciplinary communication\n\n\n\n\n\n\n\n\nKonstantinos Kyritsis\nCo-founder / COO, Bioinformatician\nDr. Kyritsis is working as a postdoctoral researcher at the Institute of Applied Biosciences in the Centre for Research and Technology Hellas (INAB/CERTH). His research interests are currently focused on studying molecular mechanisms and unveiling patterns of functional significance using machine learning algorithms to omics data. Furthermore, his interests include developing bioinformatic tools and pipelines for integrating and visualizing heterogenous bio-data.\n \n\n\nNikos Pechlivanis\nCo-founder / CTO, Bioinformatics Engineer\nNikos Pechlivanis works as a Bioinformatics engineer at the Institute of Applied Biosciences in the Centre for Research and Technology Hellas. His research interests focuses on studying machine learning algorithms and computational techniques with respect to their corresponding applications on genomic and meta-genomic data. In addition to his research, he supports open source/science principles, in support of reusability and accessibility of scientific results.\n\n\n\n\n\n\n\n\nFotis Psomopoulos\nCo-founder/ CSO, ML engineer & Research Manager\nDr. Fotis Psomopoulos is a Senior Researcher (Associate Professor level) at the Institute of Applied Biosciences (INAB), at the Centre for Research and Technology Hellas (CERTH), in Thessaloniki Greece. His research interests lie at the intersection of Bioinformatics and Machine Learning, primarily working on the design and implementation of data mining algorithms for knowledge extraction from large datasets in Life Sciences. He is the Training Coordinator of ELIXIR-GR, a member of the ELIXIR Training Platform Executive Committee, and a member of the EOSC Association Research Careers, Recognition and Credit Task Force. Finally, he is a strong advocate of FAIR and Open Science; as applied in research software and machine learning in particular. He is a co-author of the Open Science Training Handbook and the Greek National Plan for Open Science, and is leading the ELIXIR Software Best Practices group, with a notable outcome the ELIXIR Software Management Plan, and the RDA FAIR for Machine Learning Interest Group."
  },
  {
    "objectID": "about-rwds.html",
    "href": "about-rwds.html",
    "title": "About",
    "section": "",
    "text": "Welcome to the home of Real World Data Science, a new project from the Royal Statistical Society. This site and its content are being developed by data science practitioners and leaders with a single goal in mind: to help you deliver high quality, ethical, impactful data science in your workplace."
  },
  {
    "objectID": "about-rwds.html#what-are-our-aims",
    "href": "about-rwds.html#what-are-our-aims",
    "title": "About",
    "section": "What are our aims?",
    "text": "What are our aims?\nReal World Data Science aims to be a trusted, go-to source for high-quality, engaging and inspiring content which helps data science students, practitioners and leaders to:\n\ndiscover and learn more efficiently;\n\nacquire practical problem-solving skills;\n\nshare their knowledge and accomplishments publicly;\n\nwork smarter, ethically, and more effectively."
  },
  {
    "objectID": "about-rwds.html#what-we-provide",
    "href": "about-rwds.html#what-we-provide",
    "title": "About",
    "section": "What we provide",
    "text": "What we provide\nResources are created to meet the needs of our target audiences. These include:\n\nCase studies – showing how data science is used to solve real-world problems in business, public policy and beyond.\nExplainers – interrogating the underlying assumptions and limitations of data science tools and methods, to help data scientists make smarter, more informed analytical choices.\nExercises – to challenge and develop the analytical mindset that all data scientists need to succeed.\n\nAdvice – interviews, Q&As, and FAQs on such topics as data science ethics, career paths, and communication, to support professional development.\n\nWe are also curating resources to help data scientists identify trustworthy, high-quality content. These include:\n\nTraining guides – step-by-step approaches and recommended sources for learning new skills and methods.\nDatasets – tagged and sorted to help educators and practitioners find data to meet their teaching and training needs.\nFeeds – who and what to follow to keep up with new ideas and developments."
  },
  {
    "objectID": "about-rwds.html#how-you-can-get-involved",
    "href": "about-rwds.html#how-you-can-get-involved",
    "title": "About",
    "section": "How you can get involved",
    "text": "How you can get involved\nSee our open call for contributions."
  },
  {
    "objectID": "case-studies/posts/2023/06/15/road-to-reproducible-research.html",
    "href": "case-studies/posts/2023/06/15/road-to-reproducible-research.html",
    "title": "The road to reproducible research: hazards to avoid and tools to get you there safely",
    "section": "",
    "text": "Reproducibility, or “the ability of a researcher to duplicate the results of a prior study using the same materials as the original investigator”, is critical for sharing and building upon scientific findings. Reproducibility not only verifies the correctness of processes leading to results but also serves as a prerequisite for assessing generalisability to other datasets or contexts. This we refer to as replicability, or “the ability of a researcher to duplicate the results of a prior study if the same procedures are followed but new data are collected”. Reproducibility, which is the focus of our work here, can be challenging – especially in the context of deep learning. This article, and associated material, aims to provide practical advice for overcoming these challenges.\nOur story begins with Davit Svanidze, a master’s degree student in economics at the London School of Economics (LSE). Davit’s efforts to make his bachelor’s thesis reproducible are what inspires this article, and we hope that readers will be able to learn from Davit’s experience and apply those learnings to their own work. Davit will demonstrate the use of Jupyter notebooks, GitHub, and other relevant tools to ensure reproducibility. He will walk us through code documentation, data management, and version control with Git. And, he will share best practices for collaboration, peer review, and dissemination of results.\nDavit’s story starts here, but there is much more for the interested reader to discover. At certain points in this article, we will direct readers to other resources, namely a Jupyter notebook and GitHub repository which contain all the instructions, data and code necessary to reproduce Davit’s research. Together, these components offer a comprehensive overview of the thought process and technical implementation required for reproducibility. While there is no one-size-fits-all approach, the principles remain consistent."
  },
  {
    "objectID": "case-studies/posts/2023/06/15/road-to-reproducible-research.html#davits-journey-towards-reproducibility",
    "href": "case-studies/posts/2023/06/15/road-to-reproducible-research.html#davits-journey-towards-reproducibility",
    "title": "The road to reproducible research: hazards to avoid and tools to get you there safely",
    "section": "Davit’s journey towards reproducibility",
    "text": "Davit’s journey towards reproducibility\n\nMore power, please\nThe focus of my bachelor’s thesis was to better understand the initial spread of Covid-19 in China using deep learning algorithms. I was keen to make my work reproducible, but not only for my own sake. The “reproducibility crisis” is a well-documented problem in science as a whole,1 2 3 4 with studies suggesting that around one-third of social science studies published between the years 2010 and 2015 in top journals like Nature and Science could not be reproduced.5 Results that cannot be reproduced are not necessarily “wrong”. But, if findings cannot be reproduced, we cannot be sure of their validity.\nFor my own research project, I gathered all data and started working on my computer. After I built the algorithms to train the data, my first challenge to reproducibility was computational. I realised that training models on my local computer was taking far too long, and I needed a faster, more powerful solution to be able to submit my thesis in time. Fortunately, I could access the university server to train the algorithms. Once the training was complete, I could generate the results on my local computer, since producing maps and tables was not so demanding. However…\n\n\nBloody paths!\nIn switching between machines and computing environments, I soon encountered an issue with my code: the paths, or file directory locations, for the trained algorithms had been hardcoded! As I quickly discovered, hardcoding a path can lead to issues when the code is run in a different environment, as the path might not exist in the new environment.\nAs my code became longer, I overlooked the path names linked to algorithms that were generating the results. This mistake – which would have been easily corrected if spotted earlier – resulted in incorrect outputs. Such errors could have enormous (negative) implications in a public health context, where evidence-based decisions have real impacts on human lives. It was at this point that I realised that my code is the fundamental pillar of the validity of my empirical work. How can someone trust my work if they are not able to verify it?\nThe following dummy code demonstrates the hardcoding issue:\n```{python}\n# Hardcoded path\nfile_path = \"/user/notebooks/toydata.csv\"\ntry:\n    with open(file_path) as file:\n        data = file.read()\n        print(data)\nexcept FileNotFoundError:\n    print(\"File not found\")\n```\n\nIn the code above, a dummy file (toydata.csv) is used. The dummy file contains data on the prices of three different toys, but only the path of the file is relevant to this example. If the hardcoded file path – \"/user/notebooks/toydata.csv\" – exists on the machine being used, the code will run just fine. But, when run in a different environment without said path, the code will result in a \"File not found error\". Better code that uses relative paths can be written as:\n```{python}\n# Relative path\nimport os\n\nfile_path = os.path.join(os.getcwd(), \"toydata.csv\")\ntry:\n    with open(file_path) as file:\n        data = file.read()\n        print(data)\nexcept FileNotFoundError:\n    print(\"File not found\")\n```\n\nYou can see that this code has successfully imported data from the dataset toydata.csv and printed its two columns (toy and price) and three rows.\nThe following example is a simplified version of what happened when I wrote code to train several models, store the results and run a procedure to compare results with the predictive performance of a benchmark model:\n```{python}\n# Set an arbitrary predictive performance value of a benchmark model\n# and accept/reject models if the results are above/below the value.\nbenchmark = 50\n# Set the model details in one place for a better overview\nmodel = {\n    \"model1\": {\"name\": \"model1\", \"type\": \"simple\"}, \n    \"model2\": {\"name\": \"model2\", \"type\": \"complex\"}\n}\n# Set the current model to \"model1\" to use it for training and check its results\ncurrent_model = model[\"model1\"]\n# Train a simple model for \"model1\" and a complex model for \"model2\"\n# Training result of the \"model1\" is 30 and for \"model2\" is 70\nmodel_structure = train(current_model[\"type\"])\n# Save the model and its result in a .csv file\nmodel_structure.to_csv('/all/notebooks/results-of-model1.csv', index=False)\n```\n```{python}\n# Load the model result and compare with benchmark\nprint(\"Model name: {}\".format(current_model[\"name\"]))\nprint(\"Model type: {}\".format(current_model[\"type\"]))\n# Load the result of the current model\nresult = pd.read_csv('/all/notebooks/results-of-model2.csv').iloc[0, 0]\nprint(\"Result: {}\".format(result))\n\nif result > benchmark:\n    print(\"\\033[3;32m>>> Result is better than the benchmark -> Accept the model and use it for calculations\")\nelse:\n    print(\"\\033[3;31m>>> Result is NOT better than the benchmark -> Reject the model as it is not optimal\")\n```\n\nEverything looks fine at a glance. But, if you examine the code carefully, you may spot the problem. Initially, when I coded the procedure (training the model, saving and loading the results), I hardcoded the paths and had to change them for each tested model. First, I trained model2, a complex model, and tested it against the benchmark (70 > 50 → accepted). I repeated the procedure for model1 (a simple model). Its result was identical to model2, therefore I kept model1 following the parsimony principle.\nHowever, for the code line loading the result for the current model (line 5, second cell), I forgot to amend the path and so mistakenly loaded the result of model2. As a consequence, I accepted a model which should have been rejected. These wrong results were then spread further in the code, including all charts and maps and the conclusions of my analysis.\nA small coding error like this can therefore be fatal to an analysis. Below is the corrected code:\n```{python}\nimport os\n# Set an arbitrary predictive performance value of a benchmark model\n# and accept/reject models if the results are above/below the value.\nbenchmark = 50\n# Set the model details (INCLUDING PATHS) in one place for a better overview\nmodel = {\n    \"model1\": {\"name\": \"model1\", \"type\": \"simple\", \"path\": os.path.join(os.getcwd(), \"results-of-model1.csv\")}, \n    \"model2\": {\"name\": \"model2\", \"type\": \"complex\", \"path\": os.path.join(os.getcwd(), \"results-of-model2.csv\")}\n}\n# Set the current model to \"model1\" to use it for training and check its results\ncurrent_model = model[\"model1\"]\n# Train a simple model for \"model1\" and a complex model for \"model2\"\n# Training result of the \"model1\" is 30 and for \"model2\" is 70\nmodel_structure = train(current_model[\"type\"])\n# Save the model and its result in a .csv file\nmodel_structure.to_csv(current_model[\"path\"], index=False)\n```\n```{python}\n# Get the model result and compare with the benchmark\nprint(\"Model name: {}\".format(current_model[\"name\"]))\nprint(\"Model type: {}\".format(current_model[\"type\"]))\n# Load the result of the current model WITH a VARIABLE PATH\nresult = pd.read_csv(current_model[\"path\"]).iloc[0, 0]\nprint(\"Result: {}\".format(result))\n\nif result > benchmark:\n    print(\"\\033[3;32m>>> Result is better than the benchmark -> Accept the model and use it for calculations\")\nelse:\n    print(\"\\033[3;31m>>> Result is NOT better than the benchmark -> Reject the model as it is not optimal\")\n```\n\nHere, the paths are stored with other model details (line 7–8, first cell). Therefore, we can use them as variables when we need them (e.g., line 16, first cell, and line 5, second cell). Now, when the current model is set to model1 (line 11, first cell), everything is automatically adjusted. Also, if the path details need to be changed, we only need to change them once and everything else is automatically adjusted and updated. The code now correctly states that model1 performs worse than the benchmark and is therefore rejected and we should keep model2, which performs best.\nI managed to catch this error in time, but it often can be difficult to spot our own mistakes. That is why making code available to others is crucial. A code review by a second (or third) pair of eyes can save everyone a lot of time and avoid spreading incorrect results and conclusions.\n\n\nSolving compatibility chaos with Docker\nOne might think that it would be easy to copy code from one computer to another and run it without difficulties, but it turns out to be a real headache. Different operating systems on my local computer and the university server caused multiple compatibility issues and it was very time-consuming to try to solve them. The university server was running on Ubuntu, a Linux distribution, which was not compatible with my macOS-based code editor. Moreover, the server did not support the Python programming language – and all the deep learning algorithm packages that I needed – in the same way as my macOS computer did.\nAs a remedy, I used Docker containers, which allowed me to create a virtual environment with all the necessary packages and dependencies installed. This way, I could integrate them with different hardware and use the processing power of that hardware. To get started with Docker, I first had to install it on my local computer. The installation process is straightforward and the Docker website provides step-by-step instructions for different operating systems. In fact, I found the Docker website very helpful, with lots of resources and tutorials available. Once Docker was installed, it was easy to create virtual environments for my project and work with my code, libraries, and packages, without any compatibility issues. Not only did Docker containers save me a lot of time and effort, but they could also make it easier for others to reproduce my work.\nBelow is an example of a Dockerfile which recreates an environment with Python 3.7 on Linux. It describes what, how, when and in which order operations should be carried out to generate the environment with all Python packages required to run the main Python script, main.py.\n\n\nAn example of a Dockerfile.\n\nIn this example, by downloading the project, including the Dockerfile, anyone can run main.py without installing packages or worrying about what OS was used for development or which Python version should be installed. You can view Docker as a great robot chef: show it a recipe (Dockerfile), provide the ingredients (project files), push the start button (to build the container) and wait to sample the results.\n\n\nWhy does nobody check your code?\nEven after implementing Docker, I still faced another challenge to reproducibility: making the verification process for my code easy enough that it could be done by anyone, without them needing a degree in computer science! Increasingly, there is an expectation for researchers to share their code so that results can be reproduced, but there are as yet no widely accepted or enforced standards on how to make code readable and reusable. However, if we are to embrace the concept of reproducibility, we must write and publish code under the assumption that someone, somewhere – boss, team member, journal reviewer, reader – will want to rerun our code. And, if we expect that someone will want to rerun our code (and hopefully check it), we should ensure that the code is readable and does not take too long to run.\nIf your code does take too long to run, some operations can often be accelerated – for example, by reducing the size of the datasets or by implementing computationally efficient data processing approaches (e.g., using PyTorch). Aim for a running time of a few minutes – or about as long as it takes to make a cup of tea or coffee. Of course, if data needs to be reduced to save computational time, the person rerunning your code won’t generate the same results as in your original analysis. This therefore will not lead to reproducibility, sensu stricto. However, as long as you state clearly what are the expected results from the reduced dataset, your peers can at least inspect your code and offer feedback, and this marks a step towards reproducibility.\nWe should also make sure our code is free from bugs – both the kind that might lead to errors in analysis and also those that stop the code running to completion. Bugs can occur for various reasons. For example, some code chunks written on a Windows machine may not properly execute on a macOS machine because the former uses \\ for file paths, while the latter uses /:\n```{python}\n# Path works on macOS/Linux\nwith open(\"../../all/notebooks/toydata.csv\", \"r\") as f:\n    print(f.read())\n\n# Path works only on Windows    \nwith open(r\"..\\..\\all\\notebooks\\toydata.csv\", \"r\") as f:\n   print(f.read())\n```\n\nHere, only the macOS/Linux version works, since the code this capture was taken from was implemented on a Linux server. There are alternatives, however. The code below works on macOS, Linux, and also Windows machines:\n```{python}\nfrom pathlib import Path\n\n# Path works on every OS: macOS/Linux/Windows\n# It will automatically replace the path to \"..\\..\\all\\notebooks\\toydata.csv\" when it runs on Windows\nwith open(Path(\"../../all/notebooks/toydata.csv\"), \"r\") as f:\n    print(f.read())\n```\n\nThe extra Python package, pathlib, is of course unnecessary if you build a Docker container for your project, as discussed in the previous section.\n\n\nJupyter, King of the Notebooks\nBy this stage in my project, I was feeling that I’d made good progress towards ensuring that my work would be reproducible. I’d expended a lot of effort to make my code readable, efficient, and also absent of bugs (or, at least, this is what I was hoping for). I’d also built a Docker container to allow others to replicate my computing environment and rerun the analysis. Still, I wanted to make sure there were no barriers that would prevent people – my supervisors, in particular – from being able to review the work I had done for my undergraduate thesis. What I wanted was a way to present a complete narrative of my project that was easy to understand and follow. For this, I turned to Jupyter Notebook.\n\n\n\n\n\n\nCredit: Discord software, Midjourney bot.\n\nJupyter notebooks combine Markdown text, code, and visualisations. The notebook itself can sit within an online directory of folders and files that contain all the data and code related to a project, allowing readers to understand the processes behind the work and also access the raw resources. From the notebook I produced, readers can see exactly what I did, how I did it, and what my results were.\nWhile creating my notebook, I was able to experiment with my code and iterate quickly. Code cells within a document can be run interactively, which allowed me to try out different approaches to solving a problem and see the results almost in real time. I could also get feedback from others and try out new ideas without having to spend a lot of time writing and debugging code.\n\n\nVersion control with Git and GitHub\nMy Jupyter notebook and associated folders and files are all available via GitHub. Git is a version control system that allows you to keep track of changes to your code over time, while GitHub is a web-based platform that provides a central repository for storing and sharing code. With Git and GitHub, I was able to version my code and collaborate with others without the risk of losing any work. I really couldn’t afford to redo the entire year I spent on my dissertation!\nGit and GitHub are great for reproducibility. By sharing code via these platforms, others can access your work, verify it and reproduce your results without risking changing or, worse, destroying your work – whether partially or completely. These tools also make it easy for others to build on your work if they want to further develop your research. You can also use Git and GitHub to share or promote your results across a wider community. The ability to easily store and share your code also makes it easy to keep track of the different versions of your code and to see how your work has evolved.\nThe following illustration shows the tracking of very simple changes in a Python file. The previous version of the code is shown on the left; the new version is shown on the right. Additions and deletions are highlighted in green and red, and with + and - symbols, respectively.\n\n\nA simple example of GitHub version tracking."
  },
  {
    "objectID": "case-studies/posts/2023/06/15/road-to-reproducible-research.html#the-deep-learning-challenge",
    "href": "case-studies/posts/2023/06/15/road-to-reproducible-research.html#the-deep-learning-challenge",
    "title": "The road to reproducible research: hazards to avoid and tools to get you there safely",
    "section": "The deep learning challenge",
    "text": "The deep learning challenge\nSo far, this article has dealt with barriers to reproducibility – and ways around them – that will apply to most, if not all, modern research projects. While I’d encourage any scientist to adopt these practices in their own work, it is important to stress that these alone cannot guarantee reproducibility. In cases where standard statistical procedures are used within statistical software packages, reproducibility is often achievable. However, in reality, even when following the same procedures, differences in outputs can occur, and identifying the reasons for this may be challenging. Cooking offers a simple analogy: subtle changes in room temperature or ingredient quality from one day to the next can impact the final product.\nOne of the challenges for research projects employing machine learning and deep learning algorithms is that outputs can be influenced by the randomness that is inherent in these approaches. Consider the four portraits below, generated by the Midjourney bot.\n\n\n\n\n\n\nCredit: Discord software, Midjourney bot.\n\nEach portrait looks broadly similar at first glance. However, upon closer inspection, critical differences emerge. These differences arise because deep learning models rely on numerous interconnected layers to learn intricate patterns and representations. Slight random perturbations, such as initial parameter values or changes in data samples, can propagate through the network, leading to different decisions during the learning process. As a result, even seemingly negligible randomness can amplify and manifest as considerable differences in the final output, as with the distinct features of the portraits.\nRandomness is not necessarily a bad thing – it mitigates overfitting and helps predictions to be generalised. However, it does present an additional barrier to reproducibility. If you cannot get the same results using the same raw materials – data, code, packages and computing environment – then you might have good reasons to doubt the validity of the findings.\nThere are many elements of an analysis in which randomness may be present and lead to different results. For example, in a classification (where your dependent variable is binary, e.g., success/failure with 1 and 0) or a regression (where your dependent variable is continuous, e.g., temperature measurements of 10.1°C, 2.8°C, etc.), you might need to split your data into training and testing sets. The training set is used to estimate the model (hyper)parameters and the testing set is used to compute the performance of the model. The way the split is usually operationalised is as a random selection of rows of your data. So, in principle, each time you split your data into training and testing sets, you may end up with different rows in each set. Differences in the training set may therefore lead to different values of the model (hyper)parameters and affect the predictive performance that is measured from the testing set. Also, differences in the testing set may lead to variations in the predictive performance scores, which in turn lead to potentially different interpretations and, ultimately, decisions if the results are used for that purpose.\nThis aspect of randomness in the training of models is relatively well known. But randomness may hide in other parts of code. One such example is illustrated below. Here, using Python, we set the seed number to 0 using np.random.seed(seed value). The random.seed() function from the package numpy (abbreviated np) saves the state of a random function so that it can create identical random numbers independently of the machine you use, and this is for any number of executions. A seed value is an initial input or starting point used by a pseudorandom number generator to generate a sequence of random numbers. It is often an integer or a timestamp. The number generator takes this seed value and uses it to produce a deterministic series of random numbers that appear to be random but can be recreated by using the same seed value. Without providing this seed value, the first execution of the function typically uses the current system time. The animation below generates two random arrays arr1 and arr2 using np.random.rand(3,2). Note that the values 3,2 indicate that we want random values for an array that has 3 rows and 2 columns.\n```{python}\nimport numpy as np\n\n#Set the seed number e.g. to 0\nnp.random.seed(0)\n# Generate random array\narr1 = np.random.rand(3,2)\n## print(\"Array 1:\")\n## print(arr1)\n\n#Set the seed number as before to get the same results\nnp.random.seed(0)\n# Generate another random array\narr2 = np.random.rand(3,2)\n## print(\"\\nArray 2:\")\n## print(arr2)\n```\n\nIf you run the code yourself multiple times, the values of arr1 and arr2 should remain identical. If this is not the case, check that the seed value is set to 0 in lines 4 and 11. These identical results are possible because we set the seed value to 0, which ensures that the random number generator produces the same sequence of numbers each time the code is run. Now, let’s look at what happens if we remove the line np.random.seed(0):\n```{python}\n#Generate random array\narr1 = np.random.rand(3,2)\n## print(\"Array 1:\")\n## print(arr1)\n\n#Generate another random array\narr2 = np.random.rand(3,2)\n## print(\"\\nArray 2:\")\n## print(arr2)\n```\n\nHere, the values of arr1 and arr2 will be different each time we run the code since the seed value was not set and is therefore changing over time.\nThis short code demonstrates how randomness that can be controlled by the seed value may affect your code. Therefore, unless randomness is required, e.g., to get some uncertainty in the results, setting the seed value will contribute to making your work reproducible. I also find it helpful to document the seed number I use in my code so that I can easily reproduce my findings in the future. If you are currently working on some code that involves random number generators, it might be worth checking your code and making all necessary changes. In our work (see code chunk 9 in the Jupyter notebook) we set the seed value in a general way, using a framework (config) so that our code always uses the same seed to train our algorithm."
  },
  {
    "objectID": "case-studies/posts/2023/06/15/road-to-reproducible-research.html#conclusion",
    "href": "case-studies/posts/2023/06/15/road-to-reproducible-research.html#conclusion",
    "title": "The road to reproducible research: hazards to avoid and tools to get you there safely",
    "section": "Conclusion",
    "text": "Conclusion\nWe hope you have enjoyed learning more about our quest for reproducibility. We have explained why reproducibility matters and provided tips for how to achieve it – or, at least, work towards it. We have introduced a few important issues that you are likely to encounter on your own path to reproducibility. In sum, we have mentioned:\n\nThe importance of having relative instead of hard-coded paths in code.\nOperating system compatibility issues, which can be solved by using Docker containers for a consistent computing environment.\nThe convenience of Jupyter notebooks for code editing – particularly useful for data science projects and work using deep learning because of the ability to include text and code in the same document and make the work accessible to everyone (so long as they have an internet connection).\nThe need for version control using, for example, Git and GitHub, which allows you to keep track of changes in your code and collaborate with others efficiently.\nThe importance of setting the seed values in random number generators.\n\nThe graphic below provides a visual overview of the different components of our study and shows how each component works with the others to support reproducibility.\n\n\n\n\n\nWe use (A) the version control system, Git, and its hosting service, GitHub, which enables a team to share code with peers, efficiently track and synchronise code changes between local and server machines, and reset the project to a working state in case something breaks. Docker containers (B) include all necessary objects (engine, data, and scripts). Docker needs to be installed (plain-line arrows) by all users (project leader, collaborator(s), reviewer(s), and public user(s)) on their local machines (C); and (D) we use a user-friendly interface (JupyterLab) deployed from a local machine to facilitate the operations required to reproduce the work. The project leader and collaborators can edit (upload/download) the project files stored on the GitHub server (plain-line arrows) while reviewers and public users can only read the files (dotted-line arrows).\nNow, it is over to you. Our Jupyter notebook provides a walkthrough of our research. Our GitHub repository has all the data, code and other files you need to reproduce our work, and this README file will help you get started.\nAnd with that, we wish you all the best on the road to reproducibility!\n\nFind more case studies\n\n\n\n\n\nAbout the authors\n\nDavit Svanidze is a master’s degree student in economics at the London School of Economics (LSE). Andre Python is a young professor of statistics at Zhejiang University’s Center for Data Science. Christoph Weisser is a senior data scientist at BASF. Benjamin Säfken is professor of statistics at TU Clausthal. Thomas Kneib is professor of statistics and dean of research at the Faculty of Business and Economic Sciences at Goettingen University. Junfen Fu is professor of pediatrics, chief physician and director of the Endocrinology Department of Children’s Hospital, Zhejiang University, School of Medicine.\n\n\n\n\n\nAcknowledgement\n\nAndre Python has been funded by the National Natural Science Foundation of China (82273731), the National Key Research and Development Program of China (2021YFC2701905) and Zhejiang University global partnership fund (188170-11103).\n\n\n\n\n\nCopyright and licence\n\n© 2023 Davit Svanidze, Andre Python, Christoph Weisser, Benjamin Säfken, Thomas Kneib, and Junfen Fu.\n\n\n  This article is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0)  International licence.\n\n\n\nHow to cite\n\nSvanidze, Davit, Andre Python, Christoph Weisser, Benjamin Säfken, Thomas Kneib, and Junfen Fu. 2023. “The road to reproducible research: hazards to avoid and tools to get you there safely.” Real World Data Science, June 15, 2023. URL"
  },
  {
    "objectID": "case-studies/posts/2023/08/21/00-food-for-thought.html",
    "href": "case-studies/posts/2023/08/21/00-food-for-thought.html",
    "title": "The Food for Thought Challenge: Using AI to support evidence-based food and nutrition policy",
    "section": "",
    "text": "There’s a saying: “You are what you eat.” Its meaning is somewhat open to interpretation, as with many such sayings, but it is typically used to make the point that if you want to be well, you need to eat well. Nutrition scientists and dieticians spend their careers trying to figure out what “eating well” looks like – the foods the human body needs, in what quantities, and how best to consume them. Their research informs advice and guidance issued by health professionals and governments. Ultimately, though, the choice of what to eat falls to us – individuals and families – and our choices are often determined by our tastes, the availability of foodstuffs in our local stores, their price and affordability.\nSo, what exactly do we eat? Answers come from a variety of sources. In the United States, there are dietary recall studies such as the National Health and Nutrition Examination Survey, which asks a sample of respondents to report their food and beverage consumption over a set period of time. There are also organisations like IRI that collect point-of-sale data from retail stores on the actual food and drink being sold to consumers. By and large, this information comes from barcodes on product packaging being scanned at checkouts, so it is often referred to as “scanner data”.\nThis data – from dietary recall studies and retail scanners – is valuable: once we know what people are eating, we can check the nutritional content of those foods and build up a picture of what the diet of a typical individual or family looks like and how it compares to the diet recommended by doctors and policymakers. And, if we know what other foodstuffs are available, how much they cost, and the nutritional value of those items, we can work out how much families need to spend, and on what, in order to eat well and, hopefully, be well.\nFiguring all this out is where something called the Purchase to Plate Crosswalk (PPC) comes in. It’s a key tool for understanding the “healthfulness of retail food purchases” and it does this by linking IRI scanner data on what people buy with data on the nutritional content of those foods, as recorded in the US Department of Agriculture’s Food and Nutrient Database for Dietary Studies (FNDDS). But there’s a catch: scanner data is collected about hundreds of thousands of food products, whereas the FNDDS has nutritional profile information for only a few thousand items. Linking these two datasets therefore gives rise to a one-to-many matching problem – a problem that takes several hundred person-hours to resolve.\nWhat if machine learning can help? That question inspired a competition, the Food for Thought Challenge, organized by the Coleridge Initiative, a nonprofit organization working with governments to ensure that data are more effectively used for public decision-making. Researchers and data scientists were invited to use machine learning and natural language processing to more efficiently link data on supermarket products to nutrient databases.\nThis collection of articles tells the story of the Food for Thought Challenge. We begin by exploring the policy issues that drive the development of the PPC – the need to understand the national diet, developing healthy diet plans, and costing up those plans – and the issues posed by record linkage. Next, we learn about the nature of the challenge and the structure of the competition in more detail, and then the three winning teams walk us through their solutions. We end the collection with some closing thoughts on the value of competitions for addressing data scientific challenges in the public sector.\n\n\n\n\nFind more case studies\n\n\n\n\nPart 1: The Purchase to Plate Suite →\n\n\n\n\n\n\n\n\nAbout the authors\n\nBrian Tarran is editor of Real World Data Science, and head of data science platform at the Royal Statistical Society.\n\n\nJulia Lane is a professor at the NYU Wagner Graduate School of Public Service and a NYU Provostial Fellow for Innovation Analytics. She co-founded the Coleridge Initiative, whose goal is to use data to transform the way governments access and use data for the social good through training programs, research projects and a secure data facility. She recently served on the Advisory Committee on Data for Evidence Building and the National AI Research Resources Task Force.\n\n\n\n\n\nCopyright and licence\n\n© 2023 Royal Statistical Society and Julia Lane\n\n\n  This article is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0)  International licence. Thumbnail photo by Melanie Lim on Unsplash.\n\n\n\nHow to cite\n\nTarran, Brian, and Julia Lane. 2023. “The Food for Thought Challenge: Using AI to support evidence-based food and nutrition policy.” Real World Data Science, August 21, 2023. URL"
  },
  {
    "objectID": "case-studies/posts/2023/08/21/01-purchase-to-plate.html",
    "href": "case-studies/posts/2023/08/21/01-purchase-to-plate.html",
    "title": "Food for Thought: The importance of the Purchase to Plate Suite",
    "section": "",
    "text": "Disclaimer\n\n\n\nThe findings and conclusions in this publication are those of the authors and should not be construed to represent any official USDA or US Government determination or policy. This research was supported by the US Department of Agriculture’s Economic Research Service and Center for Nutrition, Policy and Promotion. Findings should not be attributed to Circana (formerly IRI).\nAbout 600,000 deaths per year in the United States are related to chronic diseases that are linked to poor dietary choices. Many other individuals suffer from diet-related health conditions, which may limit their ability to work, learn, and be physically active (US Department of Agriculture and US Department of Health and Human Services 2020). In recognition of the link between diet and health, in 1974 the Senate Select Committee on Nutrition and Human Needs, originally formed to eliminate hunger, expanded its focus to improving eating habits, nutrition policy and the national diet. Since 1980, the Dietary Guidelines for Americans have been released every five years by the US Departments of Agriculture (USDA) and Health and Human Services (DHHS). The guidelines present “advice on what to eat and drink to meet nutrient needs, promote health, and prevent disease”.\nBecause there can be economic and social barriers to maintaining a healthy diet, USDA promotes Food and Nutrition Security so that everyone has consistent and equitable access to healthy, safe, and affordable foods that promote optimal health and well-being. A set of data tools called the Purchase to Plate Suite (PPS) supports these goals by enabling the update of the Thrifty Food Plan (TFP), which estimates how much a budget-conscious family of four needs to spend on groceries to ensure a healthy diet. The TFP market basket – consisting of the specific amounts of various food categories required by the plan – forms the basis of the maximum allotment for the Supplemental Nutrition Assistance Program (SNAP, formerly known as the “Food Stamps” program), which provided financial support towards the cost of groceries for over 41 million individuals in almost 22 million households in fiscal year 2022.\nThe 2018 Farm Act (Agriculture Improvement Act of 2018) requires that USDA reevaluate the TFP every five years using current food composition, consumption patterns, dietary guidance, and food prices, and using approved scientific methods. USDA’s Economic Research Service (ERS) was charged with estimating the current food prices using retail food scanner data (Levin et al. 2018; Muth et al. 2016) and utilized the PPS for this task. The most recent TFP update was released in August 2021 and the revised cost of the market basket was the first non-inflation adjustment increase in benefits for SNAP in over 40 years (US Department of Agriculture 2021).\nThe PPS combines datasets to enhance research related to the economics of food and nutrition. There are four primary components of the suite:\nThe PPC allows researchers to measure the healthfulness of store purchases. On average US consumers acquire about 75% of their calories from retail stores, and there are a number of studies linking the availability of foods at home to the healthfulness of the overall diet (e.g., Gattshall et al. 2008; Hanson et al. 2005). Thus, understanding the healthfulness of store purchases allows us to understand differences in consumers who purchase healthy versus less healthy foods, and may contribute to better policies that promote healthier food purchases. While healthier diets are linked to a lower risk of disease outcomes (Reedy et al. 2014), other factors such as health care access may also be contributors (Cleary, Liu, and Carlson 2022). The PPC also forms the basis of the price tool, PPPT – which allows researchers to estimate custom prices for dietary recall studies – and a new ERS data product, the PP-NAP. The national average prices from PP-NAP are used in reevaluating the TFP. By using the PP-NAP with 24-hour dietary recall information from surveys such as What We Eat in America (WWEIA) – the dietary component of the nationally representative National Health and Nutrition Examination Survey(NHANES)1 – researchers can examine the relationship between the cost of food, dietary intake, and chronic diseases linked to poor diets. The price estimates also allow researchers to develop cost-effective healthy diets such as MyPlate Kitchen. The final component of the Purchase to Plate Suite, the ingredient tool (PPIT), breaks dietary recall-reported foods back into purchasable ingredients, based on US retail food purchases. The PPIT is also used in the revaluation of the TFP, and by researchers who want to look at the relationship between reported ingestion of grocery items, cost and disease outcomes using WWEIA/NHANES. More information on the development of the PPC is available in two papers by Carlson et al. (2019, 2022).\nThe Food for Thought competition aimed to support the development of the PPC – and thus policy-oriented research – by linking retail food scanner data to the USDA nutrition data used to analyze NHANES dietary recall data, specifically the Food and Nutrient Database for Dietary Studies (FNDDS) (2018, 2020). In particular, the competition set out to use artificial intelligence (AI) to reduce human resources in creating the links for the PPC, while still maintaining the high-quality standards required for reevaluating the TFP and for data published by ERS (which is one of 13 Principle Statistical Agencies in the United States Federal Government)."
  },
  {
    "objectID": "case-studies/posts/2023/08/21/01-purchase-to-plate.html#methods-used-to-date",
    "href": "case-studies/posts/2023/08/21/01-purchase-to-plate.html#methods-used-to-date",
    "title": "Food for Thought: The importance of the Purchase to Plate Suite",
    "section": "Methods used to date",
    "text": "Methods used to date\nOn the surface, the linking process may appear simple: both the FNDDS and retail food scanner data are databases of food. But the scanner data are produced for market research, and the FNDDS for dietary studies. The scanner data include about 350,000 items with sales each year, while the FNDDS has only 10,000–15,000 items. Scanner data relates to specific products, while FNDDS items are often more general. Both datasets have different hierarchical structures – the FNDDS hierarchy is based around major food groups: dairy; meat, poultry and seafood; eggs; nuts and legumes; grains; fruits; vegetables; fats and oils; and sugars, sweets, and beverages. Items fall into the groups regardless of preparation method or form. That is, broccoli prepared from frozen and from fresh both appear in the vegetable group, and for some fruits and vegetables, the fresh, frozen, canned and dried form are the same FNDDS item. Vegetable-based mixed dishes, such as broccoli and carrot stir-fry or soup, are also classified in the vegetable group. On the other hand, the scanner data classifies foods by grocery aisle. That is, the fresh and frozen broccoli are classified in different areas: produce and frozen vegetables. Similarly, when sold as a prepared food, the broccoli and carrot stir-fry may be found in the frozen entries, as a kit in either the frozen or produce section, refrigerated foods, or all of these.\nTo allow researchers to import the FNDDS nutrient data into the scanner data, a one-to-many match between FNDDS and scanner data items was needed. The food descriptions in the scanner data include brand names and package sizes and are written as a consumer would pronounce them – e.g., fresh and crisp broccoli florets, ready-cut, 10 oz – versus a more general FNDDS description such as “Broccoli, raw”. (Also linked to the “Broccoli, raw” code would be broccoli sold with stems attached, broccoli spears, and any other way raw broccoli is sold.) In the scanner data, the Universal Product Code (UPC) and the European Article Number (EAN) can link items between tables within the scanner data, as well as between datasets of grocery items, such as the USDA Global Branded Foods Product Database, a component of USDA’s Food Data Central. However, these codes are not related to the FNDDS codes, or any other column within the FNDDS. In other words, before development of the PPC, there were no established linking identifiers.\nFigure 1 shows the process USDA uses to develop matches between scanner data and FNDDS.\n\n\n\n\n\n\nFigure 1: Process currently used to create the matches between the USDA Food and Nutrient Database for Dietary Studies (FNDDS) and the retail scanner data (labelled “IRI” for the IRI InfoScan and Consumer Network) product dictionaries. Source: Author provided.\n\nWe start the linking process by categorizing the scanner data items into homogeneous groups to make the first round of automated matching more efficient. To save time, we use the second lowest hierarchical category in the scanner data which generally divides items within a grocery aisle into homogenous groups such as produce, canned beans, baking mixes, and bread. Once the linking categories for scanner data are established, we select appropriate items from the FNDDS. Since the FNDDS is highly structured, this selection is usually straightforward.\nOur next step is to use semantic matching to create a search table that aligns similar terms within the IRI product dictionary and FNDDS. This first requires that we extract attributes from the FNDDS descriptions into fields similar to those in the scanner data product dictionary. The FNDDS descriptions are found across multiple columns because they are added as the need arises to provide examples of brand names or alternative descriptions of foods which help code the foods WWEIA participants report eating. We manually create matching tables that link terms used in FNDDS to those used in the scanner data, organized by the fields defined in the restructured FNDDS. We then use this table as the basis of a probabilistic matching process. For example, when linking the produce group, “fresh” in the scanner data would be aligned with “raw” and “prepared from fresh” and NOT “prepared from frozen” in the FNDDS, and “broccoli florets” would also be aligned with “raw” and “broccoli”. Since the FNDDS is designed to code the foods individuals report eating, many of the foods in the FNDDS are already prepared and result in descriptions such as “broccoli, steamed, prepared from fresh” or “broccoli, boiled, prepared from frozen”.\nOnce the linking table is established, the probabilistic match process returns the single best possible match for each item in the scanner data. For example, a match between fresh broccoli florets and frozen broccoli would have a lower probability score than “broccoli, raw”. Because these matches form the basis of major USDA policies, we cannot accept an error rate of more than 5 percent, and lower is preferred. To reach that goal, nutritionists review every match to make sure the probabilistic match did not return a match between cauliflower florets and fresh broccoli, say, or that a broccoli and carrot stir-fry is not matched to a dish with broccoli, carrots, and chicken. The correct matches, such as the one between fresh broccoli florets and raw broccoli, are set aside while the items with an incorrect match, such as cauliflower florets and the broccoli and carrot stir-fry, are used to revise the search table. Revisions might include adding (NOT chicken) to the broccoli and carrot stir-fry dish. Mixed dishes — such as the broccoli and carrot stir-fry — pose particular challenges because there are a wide variety of similar products available in the grocery store. After a few rounds of revising the search table and running the probabilistic match process, it is more efficient to use a manual match, established by one nutritionist and reviewed by another, after which the match is assumed to be correct.\nThe process improved with each new wave of FNDDS and IRI data. Our first creation of the PPC linked the FNDDS 2011/12 to the 2013 IRI retail scanner data. Subsequent waves started with the previous search table and resulting matches were reviewed by nutritionists. We also used more fields in the IRI product dictionary to create the homogeneous linking groups and made modifications to these groups with each wave. During each wave we experimented with the number of rounds of probabilistic matching that was the most cost effective. For some linking groups it took less human time to manually match from the start, while for other groups it was more efficient to do multiple rounds of improvements to the search table. Starting with the most recent wave (matching FNDDS 2017/18 to the 2017 and 2018 retail scanner data), we assumed previous matches appearing in the newer data were correct. Although this assumption was good for most matches, a review demonstrated the need to review previous matches prior to removing the item from the list of scanner data items needing FNDDS matches. In the future we intend to explore methods developed by the participants of the Food for Thought competition."
  },
  {
    "objectID": "case-studies/posts/2023/08/21/01-purchase-to-plate.html#linking-challenges",
    "href": "case-studies/posts/2023/08/21/01-purchase-to-plate.html#linking-challenges",
    "title": "Food for Thought: The importance of the Purchase to Plate Suite",
    "section": "Linking challenges",
    "text": "Linking challenges\nAn ongoing challenge to the linking problem is that both the scanner data and the FNDDS undergo substantive changes each year, meaning that both the previous matches and search tables need to be reviewed and revised with each new effort, as tables that work with one cycle of FNDDS and scanner data will need revisions to use with the next cycle. Changes to the scanner data that impact our current method include dropped and added items, data corrections, and revisions to the categories that form the basis of the homogeneous linking groups. In addition, there are errors such as incorrect food descriptions, conflicting package size information, and changes in the item description from year to year. Since the FNDDS is designed to support dietary recall studies, revisions reflect both changes to available foods and the level of detail respondents can provide. These revisions result in dropped/added food codes, changes to food descriptions that impact which scanner data items match to the FNDDS items, and revisions to recipes used in the nutrient coding which impacts the number of retail ingredients available in the FNDDS.\nOf the four parts of the PPS, establishing the matches is the most time-consuming task and constitutes at least 60 percent of the total budget. In the most recent round, we had 168 categories and each one went through 2-3 automated matching rounds; after each round, nutritionists spent an average of two hours reviewing the matches. This adds up to somewhere between 670 and 1,000 hours of review time. After the automated review, manual matching requires an additional 300 hours. Reducing the amount of time required to establish matches and link the FNDDS and retail scanner datasets may lead to significant time savings, resulting in faster data availability. That, in turn, could allow more timely policy-based research, and the mandated revision of the Thrifty Food Plan can continue with the most recent food price data.\n\n\n\n\n← Introduction\n\n\n\n\nPart 2: Competition design →\n\n\n\n\n\n\n\n\nAbout the authors\n\nAndrea Carlson is an agricultural economist in the Food Markets Branch of the Food Economics Division in USDA’s Economic Research Service. She is the project lead for the Purchase to Plate Suite, which allows users to import USDA nutrient and food composition data into retail food scanner data acquired by USDA and estimate individual food prices for dietary intake data.\n\n\nThea Palmer Zimmerman is a senior study director and research nutritionist at Westat.\n\n\n\n\n\nImage credit\n\nThumbnail photo by Kenny Eliason on Unsplash.\n\n\n\n\n\nHow to cite\n\nCarlson, Andrea, and Thea Palmer Zimmerman. 2023. “Food for Thought: The importance of the Purchase to Plate Suite.” Real World Data Science, August 21, 2023. URL"
  },
  {
    "objectID": "case-studies/posts/2023/08/21/01-purchase-to-plate.html#acknowledgements",
    "href": "case-studies/posts/2023/08/21/01-purchase-to-plate.html#acknowledgements",
    "title": "Food for Thought: The importance of the Purchase to Plate Suite",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nThe research presented in this compendium supports the Purchase to Plate Suite of data products. Carlson has been privileged to both develop and lead this project over the course of her career, but it is not a solo project. Many thanks to the Linkages Team from USDA’s Economic Research Service (Christopher Lowe, Mark Denbaly Elina Page, and Catherine Cullinane Thomas) the Center for Nutrition Policy and Promotion (Kristin Koegel, Kevin Kuczynski, Kevin Meyers Mathieu, TusaRebecca Pannucci), and our contractor Westat, Inc. (Thea Palmer Zimmerman, Carina E. Tornow, Amber Brown McFadden, Caitlin Carter, Viji Narayanaswamy, Lindsay McDougal, Elisha Lubar, Lynnea Brumby, Raquel Brown, and Maria Tamburri). Many others have supported this project over the years."
  },
  {
    "objectID": "case-studies/posts/2023/08/21/02-competition-design.html",
    "href": "case-studies/posts/2023/08/21/02-competition-design.html",
    "title": "Food for Thought: Competition and challenge design",
    "section": "",
    "text": "Since 2014, the professional services firm Westat, Inc. has been developing the Purchase to Plate Crosswalk (PPC) for the United States Department of Agriculture (USDA) Economic Research Service (ERS). The PPC links the retail food transactions database from IRI’s InfoScan service and the USDA Food and Nutrient Database for Dietary Studies (FNDDS). However, the current linkage process uses only partly automated data matching, meaning it is resource intensive, time consuming, and requires manual review.\nWith sponsorship from ERS, Westat partnered with the Coleridge Initiative to host the Food for Thought competition to challenge researchers and data scientists to use machine learning and natural language processing to find accurate and efficient methods for creating the PPC. Figure 1 provides a visual overview of the challenge set by the competition.\nThe one-to-many matching task that is central to the competition throws up many challenges for researchers to wrestle with. Because IRI data contains food transactions collected from partnered retail establishments for over 350,000 items, the matchings need to be made based on limited data features, including categories, providers, and semantically inconsistent descriptions that consist of short phrases. Consider this hypothetical example: IRI product-related information about a (fictional) “Cheesy Hashbrowns Hamburger Helper, 5.5 Oz Box” needs to be linked to FNDDS nutrition-related information found under “Mixed dishes – meat, poultry, seafood: Mixed meat dishes”. Figure 2 demonstrates how the two databases are linked with each other to create the PPC. As can be seen, there is no common word that easily indicates that “Cheesy Hashbrowns Hamburger Helper…” should be matched with “Mixed dishes…”, and such cases exist in all IRI tables used for the challenge, from 2012 through 2018.\nAlso, because nutritionists or food scientists will always need to review the matching, regardless of the matching method used, it was important that our evaluation of proposed matching methods focused both on the accuracy of prediction models and also on metrics that would lead participants to develop models that facilitate qualified reviewers to reduce their workloads.\nOrganising the competition was also a challenge in its own right, for data privacy reasons. IRI scanner data contains sensitive information, such as store name, location, unit price, and weekly quantity sold for each item. This ruled out using existing online platforms like Kaggle, DrivenData or AIcrowd to host the competition, and instead required a private secure data enclave to ensure the safe use of sensitive and confidential data assets. The need for such an environment imposed capacity constraints on the competition, meaning only dozens of teams could be invited to take part, whereas on open platforms it is common to have thousands of teams competing and sharing ideas and code."
  },
  {
    "objectID": "case-studies/posts/2023/08/21/02-competition-design.html#competition-structure",
    "href": "case-studies/posts/2023/08/21/02-competition-design.html#competition-structure",
    "title": "Food for Thought: Competition and challenge design",
    "section": "Competition structure",
    "text": "Competition structure\nThe competition ran over 10 months and consisted of three separate challenges: two interim, one final. Applications opened in September 2021, and the competition started in January 2022. Submission deadlines for the first and second interim challenges were in July and September 2022, respectively. For these rounds, participants submitted preliminary solutions for evaluation based solely on quantitative metrics, and two awards of $10,000 were given to the highest-scoring teams. The deadline for the final challenge was in October 2022. Here, solutions were evaluated by the scientific review board based on three judging criteria: quantitative metrics, transferability, and innovation. First, second, and third place winners received awards of $30,000, $1,500, and $1,000 respectively. Final presentations were given at the Food for Thought symposium in December 2022.\nThe competition was run entirely within the Coleridge Initiative’s Administrative Data Research Facility (ADRF), which was established by the United States Census Bureau to inform the decision-making of the Commission on Evidence-Based Policy under the Evidence Act. ADRF follows the Five Safes Framework: safe projects, safe people, safe data, safe settings, and safe outputs.\nIn keeping with this framework, participants were provided with ADRF login credentials after signing the relevant data use agreements during the onboarding process. All participants were required to agree to the ADRF terms of use, to complete security training, and to pass a security training assessment prior to accessing the challenge data. Participants’ access within ADRF was limited to the challenge environment and data only. There was no internet access, so Coleridge Initiative ensured that any packages requested by teams were available for use within the environment after passing security review. All codes and documentation were only allowed to be exported outside ADRF after export reviews from both Coleridge Initiative and USDA staff. At the end of each challenge, the teams submitted write-ups and supporting files by placing all the necessary submission files in their ADRF team folder. Detailed submission instructions are available via the Real World Data Science GitHub repository."
  },
  {
    "objectID": "case-studies/posts/2023/08/21/02-competition-design.html#metrics",
    "href": "case-studies/posts/2023/08/21/02-competition-design.html#metrics",
    "title": "Food for Thought: Competition and challenge design",
    "section": "Metrics",
    "text": "Metrics\nSubmissions were evaluated by Coleridge Initiative and technical review and subject review boards based on the following criteria:\n\nQuantitative metrics were used to measure the predictive accuracy and runtime of the model.\n\nTransferability measured the quality of documentation and code, and the ability of individuals who are not involved in model development to replicate and implement the team’s approach.\n\nInnovation measured novelty and creativity of the model in addressing the linkage problem.\n\nTechnical review was overseen by faculty members from computer science and engineering departments of top US universities. Subject review was handled by subject matter experts from USDA and Westat.\nFrom a quantitative perspective, the most common way to evaluate machine learning competition submissions is to use model predictive accuracy. However, single metrics are typically incomplete descriptions of real-world tasks, and they can easily hide significant differences between models which simple predictive accuracy cannot capture. To select the most appropriate official challenge metrics, Coleridge Initiative reviewed the literature on the use of evaluation measures in both classification and ranking task machine learning competitions. Success at 5 (S@5) and Normalized Discounted Cumulative Gain at 5 (NDCG@5) scores were ultimately used as the quantitative metrics.\nThe metrics were applied as follows: models proposed by each team were tasked with outputting five potential FNDDS matches for each IRI code, with potential FNDDS matches ordered from most likely to least likely. S@5 and NDCG@5 scores are broadly similar – both measure whether a correct match is present in the five proposed matches that participants were asked to identify. However, S@5 does not take rank position into account and only considers whether the five proposed FNDDS matches contain the correct FNDDS response. NDCG@5 does take rank into account and also measures how highly the correct FNDDS response is ranked among the five proposed matches. Both measures range from 0 to 1 (or 0% to 100%). Models get a “full credit” for S@5 as long as they contain the correct FNDDS option. NDCG@5 penalizes models when the correct match is ranked lower on the list of 5 proposed matches."
  },
  {
    "objectID": "case-studies/posts/2023/08/21/02-competition-design.html#technical-description",
    "href": "case-studies/posts/2023/08/21/02-competition-design.html#technical-description",
    "title": "Food for Thought: Competition and challenge design",
    "section": "Technical description",
    "text": "Technical description\n\nEnvironment setup\nColeridge Initiative solicited technical requirements from participants at the challenge application stage to prepare the ADRF environment as much as possible before the competition began. Each team was asked to share anticipated workspace specifications and software library requests in their application package. From this we identified, reviewed, and installed the requested Python and R packages, libraries, and library components (e.g., pre-trained models, training data) that were not yet available within ADRF.\nThe setup of graphics processing units (GPUs) was also a critical part of competition preparation. We created an environment with 16 gibibyte (GiB) of GPU memory for each team. Our technology team met with multiple teams several times to discuss computing environment configurations to ensure the GPU could work properly. None of these efforts was wasted: without GPU access, it would be impossible for teams to use state-of-the-art pre-trained models such as the Bidirectional Encoder Representations from Transformers (BERT, Devlin et al. 2018).\nWe completed the setup of new team workspaces, each customized to the individual team’s resource and library requirements, including GPU configuration. The isolation and customization of workspaces was vital because teams may request different versions of libraries that potentially have version conflict with other libraries. We ensured the configurations were all set before the challenge began because such data challenges are bursty in nature (Macavaney et al. 2021), and handling support requests in the private data enclave risked causing delays. We hoped to avoid receiving too many requests in the beginning phase of the competition in order to give participants a better experience, though we did of course provide participants with instructions on how to request additional libraries during the challenge period.\n\n\nSupporting materials\nIn addition to environment preparation, we made available a list of supporting documentation, including IRI, PPC, and FNDDS codebooks, technical reports, and related publications that could help teams understand the challenge datasets. The FNDDS codebook pooled information on variable availability, coding, and descriptions across dataset files and years. It also included internal Westat food category coding difficulty ratings and notes on created PPC codes and provided UPC code, EC code, and general dataset remarks and observations that may take time for analysts to discover on their own.\nWe developed a baseline model to demonstrate the challenge task and the expected outputs – both outside of ADRF using FNDDS and fictitious data in place of IRI data, and an analogous model using FNDDS and IRI data within the ADRF secure environment. Moreover, we provided the teams with an evaluation script to read in their submissions and evaluate them for predictive accuracy against the public test set using S@5 and NDCG@5 challenge metrics. Finally, we held multiple webinars during the course of the challenge to explain next steps, address participant questions, solicit feedback, and provide general support. Multiple teams also met with our technology team to clarify ADRF-related questions or troubleshoot technical issues.\n(Baseline model, toolkits, and evaluation script are available from the Real World Data Science GitHub repository.)\n\n\nData splitting\nTo mimic the real-world scenario, the competition used 2012–2016 IRI data as the training set, and the 2017–2018 IRI data as the test set, since the data change over time and USDA could provide the most recent data available. To make sure that models were generalizable and not just overfit to the test set, we split the test set into private and public test sets. In this way, we guaranteed that the models were evaluated on completely hidden data. In order to keep the similar distribution of the two sets, we first divided the data into five quintiles based on EC code frequencies and then randomly sampled 80% of records in each group without repetition for placement into the private test set. Later in the competition, because of the computation limit, we further shrank the private test set to 40% of its original size using the same data-splitting method.\n\n\nJudging\nIn the first two rounds, submissions were evaluated based on the quantitative metrics, as previously mentioned above. Coleridge Initiative was responsible for running the evaluation script, making sure not to re-train the model or modify the configs in any way, and only applying the model to predict the private test set. Prediction results were then compared against ground truth to get the private scores.\nThe final challenge was reviewed by the scientific review board on all three judging criteria. Submitted models were first evaluated by Coleridge Initiative in the same way as in the first two rounds. The runtime of models was also recorded as an assessment of model cost. The scientific review boards then assessed the models by the quality of documentation, the quality of code, and the ability to replicate and implement the team’s approach, and scored the models for innovation and creativity in addressing the linkage problem. Lastly, scores were summarized and the scientific review board discussed and decided the winners of the competition."
  },
  {
    "objectID": "case-studies/posts/2023/08/21/02-competition-design.html#results",
    "href": "case-studies/posts/2023/08/21/02-competition-design.html#results",
    "title": "Food for Thought: Competition and challenge design",
    "section": "Results",
    "text": "Results\nThe next few articles in this collection walk readers through the solutions proposed by competition finalists. Figure 3 provides a brief summary.\n\n\n\n\n\n\nFigure 3: Top competitors and their solutions to the Food for Thought challenge."
  },
  {
    "objectID": "case-studies/posts/2023/08/21/02-competition-design.html#lessons-learned",
    "href": "case-studies/posts/2023/08/21/02-competition-design.html#lessons-learned",
    "title": "Food for Thought: Competition and challenge design",
    "section": "Lessons learned",
    "text": "Lessons learned\nIt was undoubtedly challenging for teams to work with highly secured data in a private data enclave for this data challenge. We solicited feedback from teams and summarized the issues that we experienced throughout the competitions, together with the solutions to resolve those issues. Below are our main lessons learned and we hope this summary can serve to inform future competitions.\n\nEnvironmental factors: The installation and setup of packages, libraries, and resources, as well as the configuration of GPUs, system dependencies, and workspace design were expected to take a long time as each team had their own needs. To accelerate the process, we requested a list of specific package and environment requirements from the teams in advance. However, due to the complexity of the system configuration required by the teams, environment setup took longer than expected. Thus, the challenge deadlines had to be postponed a few times to accommodate this.\nTime commitment: Twelve teams were selected to participate in the challenge, but only three teams remained in the final challenge. Other than one team that was disqualified for violating the ADRF terms of use agreement, eight dropped out because of other commitments and insufficient time to meaningfully participate. To ensure security, ADRF does not allow jobs to run in the backend, which also adds to the time commitment of teams. To encourage teams to participate in the final challenge, we gave out additional awards for second and third places.\nComputing resource limit: One issue encountered in evaluating submitted models was computing environment resource limits due to the secured nature of the data enclave. The original private test dataset is four times larger than the public test dataset, making it unfeasible to evaluate. To overcome this issue, given the fixed resource constraints, we decided to reduce the private test set to 40% of its original size. It would have been helpful, though, if the competition had set a model running time limit at the outset, so that participants could build simpler yet effective models.\nSupporting code: Although the initial baseline model we provided was extremely simple, we found this helped participants a lot in the initial phase – yet there is space to improve. To be specific, supporting codes should be constructed so that all relevant data tables are used and specify the main function to run the code, especially how the model should be tested. The teams only used the main table, which was the only table that was used in the baseline model, for training and did not touch the other supporting table. If we included the other table in the baseline model, it could help participants to have a better use of this data as well. In addition, a baseline model should be intuitive for the participants to follow, allowing evaluators to easily replace the public test set with the private test set without any programming modifications.\n\n\n\n\n\n← Part 1: Purchase to Plate\n\n\n\n\nPart 3: First place winners →\n\n\n\n\n\n\n\n\nAbout the authors\n\nZheyuan Zhang and Uyen Le are research scientists at the Coleridge Initiative.\n\n\n\n\n\nCopyright and licence\n\n© 2023 Zheyuan Zhang and Uyen Le\n\n\n  This article is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0)  International licence.\n\n\n\nHow to cite\n\nZhang, Zheyuan, and Uyen Le. 2023. “Food for Thought: Competition and challenge design.” Real World Data Science, August 21, 2023. URL"
  },
  {
    "objectID": "case-studies/posts/2023/08/21/03-first-place-winners.html",
    "href": "case-studies/posts/2023/08/21/03-first-place-winners.html",
    "title": "Food for Thought: First place winners – Auburn Big Data",
    "section": "",
    "text": "The Auburn Big Data team from Auburn University consists of five members, including three assistant professors: Dr Wenying Li of the Department of Agricultural Economics and Rural Sociology, Dr Jingyi Zheng of the Department of Mathematics and Statistics, and Dr Shubhra Kanti Karmaker of the Department of Computer Science and Software Engineering. Additionally, the team comprises two PhD students, Naman Bansal and Alex Knipper, who are affiliated with Dr Karmaker’s big data lab at Auburn University.\nIt is estimated that our team has spent approximately 1,400 hours on this project."
  },
  {
    "objectID": "case-studies/posts/2023/08/21/03-first-place-winners.html#our-perspective-on-the-challenge",
    "href": "case-studies/posts/2023/08/21/03-first-place-winners.html#our-perspective-on-the-challenge",
    "title": "Food for Thought: First place winners – Auburn Big Data",
    "section": "Our perspective on the challenge",
    "text": "Our perspective on the challenge\nAt the start of this competition, we decided to test three general approaches, in the order listed:\n\nA heuristic approach, where we use only the data and a defined similarity metric to predict which FNDDS label a given IRI item should have.\nA simpler modeling approach, where we train a simple statistical classifier, like a random forest (Parmar, Katariya, and Patel 2019), logistic regression, etc., to predict the FNDDS label for a given IRI item. For this method, we opted to use a random forest as our statistical model, as it was a simpler model to use as a baseline, having shown decent performance in a wide range of classification tasks. As it turned out, this approach was quite robust and accurate, so we kept it as our main model for this approach.\nA large language modeling approach, where we train a model like BERT (Devlin et al. 2018) to map the descriptions for given IRI and FNDDS items to the FNDDS category the supplied IRI item belongs to."
  },
  {
    "objectID": "case-studies/posts/2023/08/21/03-first-place-winners.html#our-approach",
    "href": "case-studies/posts/2023/08/21/03-first-place-winners.html#our-approach",
    "title": "Food for Thought: First place winners – Auburn Big Data",
    "section": "Our approach",
    "text": "Our approach\nAs we explored the data provided, we opted to use the given 2017–2018 PPC dataset as our primary dataset for both training and testing. To ensure a fair evaluation of the model, we randomly split the dataset into 60% training samples and 40% testing samples, making sure our training process never sees the testing dataset. For evaluating our models, we adopted the competition’s metrics: Success@5 and NDCG@5. After months of testing, our statistical classifier (approach #2) proved itself to be the model that both processes the data fastest and achieves the highest performance on our testing metrics.\nThis approach, at a high level, takes in the provided data (among other configuration parameters), formats the data in a computer-readable format – converting the IRI and FNDDS descriptions to a numerical representation with word embeddings (2018; Mikolov et al. 2013; Pennington, Socher, and Manning 2014) and then using that numerical representation to calculate the distances between each description – and then trains a classification model (random forest (2019)/neural network (Schmidhuber 2015)) that can predict an FNDDS label for a given IRI item.\nIn terms of data, our approach uses the FNDDS/IRI descriptions, combining them into a single “description” field, and the IRI item’s categorical items – department, aisle, category, product, brand, manufacturer, and parent company – to further discern between items.\nWhile most industrial methods require use of a graphics processing unit (graphics card, or GPU) to perform this kind of processing, our primary method only requires the computer’s internal processor (CPU) to function properly. With that in mind, to achieve the best possible performance on our test metrics, the most time-consuming operations are run in parallel. The time taken to train our primary model can likely be further improved if we parallelize these operations across a GPU, with the only downside being the imposition of a GPU requirement for systems aiming to run this method.\nIn addition to our primary method, our team has worked with alternate approaches on the GPU (using BERT (2018), neural networks (2015), etc.) to either: 1) speed up the time it takes to process and make inferences for the data, achieving similar performance on our test metrics, or 2) achieve higher performance, likely at a cost to the time it takes to process everything. Our reasoning behind doing so is that if a simple statistical model performs well, then a larger language model should be able to demonstrate a higher performance on our test metrics without much of an increase in training time. At the current time, these methods are still unable to match the performance/efficiency tradeoff of our primary method.\nAfter exploring alternate methods to no avail, our team then decided to focus again on our primary method, the random forest (2019), and a secondary method, feed-forward neural network mapping our input features (X) to the FNDDS labels (Y) (2015), to optimize their training hyperparameters for the dataset. Our aim in this is to see which of our already-implemented, easier-to-run downstream methods would better optimize the performance/efficiency tradeoff after having its training parameters optimized to the fullest. This has resulted in a marginal increase in training time (+20-30 minutes) and a roughly 5% increase in performance for our still-highest performing model, the random forest.\nOverall, our primary method – the random forest – gave us an approximate training time (including data pre-processing) of 4 hours 30 minutes for our ~38,000 IRI item training set, and an approximate inference time of 15 minutes on our testing set of ~15,000 IRI items. Furthermore, our method gave us a Success@5 score of .789 and an NDCG@5 score of .705 on our testing set.\n\nKey features\nHere is a list of the key features we utilize, along with what type of data we treat it as.\n\nFNDDS\n\nfood_code – identifier\nmain_food_description – text\nadditional_food_description – text\ningredient_description – text\n\nIRI\n\nupc – identifier\nupcdesc – text\ndept – categorical\naisle – categorical\ncategory – categorical\nproduct – categorical\nbrand – categorical\nmanufacturer – categorical\nparent – categorical\n\n\nThe intuition behind using these particular features is that the text-based descriptions provide the majority of the “meaning” of the item. By converting each description to a numerical representation (2013; 2014), we can then calculate the similarity between each “meaning” to determine which FNDDS label is most similar to the IRI item provided. However, that alone is not enough. The categorical features on the IRI item help to further enhance the model’s classifications using the logic and categories people use in places like grocery stores. For example, if given an item whose aisle was “fruit” and brand was “Dole”, the item could be reasonably expected to be something like “peaches” over something like “broccoli”.\n\n\nFeature selection\nAforementioned intuition aside, our feature selection was rather naive, in that we manually examined the data and removed any redundant text features before doing anything else. After that, we decided to use description fields as “text” data to comprise the main “meaning” of the item, represented numerically after converting the text using a word embedding (2013; 2014). We also decided to use the non-description fields (aisle, category, etc.) as “categorical” data that would be turned into its own numerical representation, allowing our model to more easily discern between items using similar systems to people.\n\n\nFeature transformations\nOur feature transformations are also relatively simple. First, we combine all description fields for each item to make one large description, and then use a word embedding method (like GloVe (2014) or BERT (2018)) to convert the description into a numerical representation, resulting in a 300-dimensional GloVe or 768-dimensional BERT vector of numbers for each description. Then, for each IRI item, we calculate the cosine and Euclidean distances from each FNDDS item, resulting in two vectors, both equal in length to the original FNDDS data (in this case, two vectors of length ~7,300). The intuition behind this is that while cosine and Euclidean distances can tell us similar things, providing both of these sets of distances to the model should allow it to pick up on a more nuanced set of relationships between the IRI and FNDDS items.\nFor categorical data, we take all unique values in each field and assign them an ID number. While that is often not the best practice for making a numerical representation out of categorical data (Potdar, Pardawala, and Pai 2017), it seemed to work for the downstream model.\nRegardless, the aforementioned feature transformations give us (ad hoc) ~14,900 features if we use GloVe and ~15,300 features if we use BERT. Both feature sets can then be sent to the downstream random forest/neural network to start classifying items.\nIt should be noted that processing the data is by far the most time-consuming part of our method. The data processing times for each embedding are as follows:\n\nGloVe: ~3 hours\nBERT: ~6 hours\n\nDue to BERT both taking so long to process data and performing lower than our GloVe embeddings on the classification task, we opt to use GloVe embeddings for our primary method. Our only theoretical explanation here is that since BERT is better at context-dependent tasks (Wang, Nulty, and Lillis 2021), it likely will expect something similar to well-structured sentences as input, which is not what the IRI/FNDDS descriptions are. Rather, GloVe – being a method that depends less on context (2013; 2014) – should excel better when the input text is not a well-formed sentence.\n\n\nTraining methods\nOnce the data has been processed, we collect the following data for each IRI item:\n\nUPC code\nDescription (converted to numerical representation)\nCategorical variables (converted to numerical representation)\nDistances to each FNDDS item\n\nOnce that has been collected for each IRI item, we can finally use our classification model. We initialize our model and begin the training process with the IRI data mentioned above and the target FNDDS labels for each one, so the model knows what the “correct” answer is for the given data. Once the model has trained on our training dataset, we save the model and it is ready for use.\nThis part of training takes much less time than preparing the data, since calculating the embeddings takes a lot more computation than a random forest model. The training times for each method are as follows:\n\nRandom Forest: ~1 hour 15 minutes\nNeural Network: ~25 minutes\n\nDespite the neural network taking far less time to train than the random forest, it still scores lower on the scoring metrics than the random forest, so we opt to continue using the random forest model as our primary method.\n\n\nGeneral approach to developing the model\nSince the linkage problem involves mapping tens of thousands of items to a smaller category set of a few thousand items, we decided to frame this problem as a multi-class classification problem (Aly 2005), where we then rank the top “k” most probable class mappings, as requested by the competition ruleset.\nMost of the usable data available to us is text data, so we need a method that can use that text-based information to accurately map classes based on the aforementioned text information. To best accomplish this, we opt to use word embedding techniques to calculate an average numerical representation for each text description (both IRI and FNDDS), so we can calculate distances between each description, giving our model a sense of how similar each description is.\n\n\nThe key “trick” to the model\nSince text descriptions hold the most information that can be used to link between an IRI item and an FNDDS item, finding a way to calculate the similarity between each description is paramount to making this method work.\nBoth distance calculation methods used in this work, cosine and Euclidean distance, are very similar in the type of information encoded, the only major difference being that cosine distance is implicitly normalized and Euclidean distance is not (Qian et al. 2004).\n\n\nNotable observations\nJust by building the ranking using the cosine similarities between each IRI item and all FNDDS items, we can achieve a Success@5 performance of 0.234 and an NDCG@5 performance of 0.312. The other features are provided and the random forest classifier is used to add some extra discriminative power to the model.\n\n\nData disclaimer\nOur current method only uses the data readily available from the 2017–2018 dataset, which we acknowledge is intended for testing. To remedy this, we further split this dataset into train/test sets and report results on our unseen test subset for our primary performance metrics. This gives a decent look into how the model will perform on unseen data.\n\n\n\n\n\n\nFind the code in the Real World Data Science GitHub repository."
  },
  {
    "objectID": "case-studies/posts/2023/08/21/03-first-place-winners.html#our-results",
    "href": "case-studies/posts/2023/08/21/03-first-place-winners.html#our-results",
    "title": "Food for Thought: First place winners – Auburn Big Data",
    "section": "Our results",
    "text": "Our results\n\nApproximate training time\nOverall, our approximate training time for our primary method is 4 hours 30 minutes broken down (approximately) as follows:\n\nReading data from database: 30 seconds\nCalculating ~7,300 FNDDS description embeddings: 15 minutes 45 seconds\nCalculating ~38,000 IRI description embeddings and similarity scores: 2 hours 20 minutes 45 seconds\nFormatting calculated data for the random forest classifier: 35 minutes\nTraining the random forest classifier: 1 hour 15 minutes\n\n\n\nApproximate inference time\nOur approximate inference time for our primary method is 15 minutes to make inferences for ~15,000 IRI items.\n\n\nS@5 & NDCG@5 performance\nThis is how our best-performing model (GloVe + random forest) performs at the current time on the testing set:\n\nNDCG@5: 0.705\nSuccess@5: 0.789\n\nWhen we evaluate that same model on the full PPC dataset we were provided (~38,000 items), we get the following scores:\n\nNDCG@5: 0.879\nSuccess@5: 0.916\n\n(Note: The full PPC dataset contains approximately 15,000 items that we used to train the model, so these scores are not as representative of our method’s performance as the previous scores.)"
  },
  {
    "objectID": "case-studies/posts/2023/08/21/03-first-place-winners.html#future-workrefinement",
    "href": "case-studies/posts/2023/08/21/03-first-place-winners.html#future-workrefinement",
    "title": "Food for Thought: First place winners – Auburn Big Data",
    "section": "Future work/refinement",
    "text": "Future work/refinement\nAs mentioned previously, we only used the given 2017–2018 PPC dataset as our primary dataset for both training and testing. Going forward, we would like to include datasets from previous years as well, which we believe would further increase our model performance. Additionally, the datasets generated from this research have the potential to inform and support additional studies from a variety of perspectives, including nutrition, consumer research, and public health. Further research utilizing these datasets has the potential to make significant contributions to our understanding of consumer behavior and the role of food and nutrient consumption in overall health and well-being."
  },
  {
    "objectID": "case-studies/posts/2023/08/21/03-first-place-winners.html#lessons-learned",
    "href": "case-studies/posts/2023/08/21/03-first-place-winners.html#lessons-learned",
    "title": "Food for Thought: First place winners – Auburn Big Data",
    "section": "Lessons learned",
    "text": "Lessons learned\nIt was interesting that the random forest model performed better than the vanilla neural network model. This shows that a simple solution can work better, depending on the application. This observation is in line with the well-established principle in machine learning that the choice of model should be guided by the nature of the problem and the characteristics of the data. In this case, the random forest model, being a simpler and more interpretable model, was better suited to the problem at hand and was able to outperform the more complex neural network model. These results underscore the importance of careful model selection and the need to consider both the complexity of the model and the specific requirements of the problem when choosing an algorithm for a particular application.\n\n\n\n\n← Part 2: Competition design\n\n\n\n\nPart 4: Second place winners →\n\n\n\n\n\n\n\n\nAbout the authors\n\nAlex Knipper and Naman Bansal are PhD students, and Jingyi Zheng, Wenying Li, and Shubhra Kanti Karmaker are assistant professors at Auburn University.\n\n\n\n\n\nCopyright and licence\n\n© 2023 Alex Knipper, Naman Bansal, Jingyi Zheng, Wenying Li, and Shubhra Kanti Karmaker\n\n\n  This article is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0)  International licence. Thumbnail photo by nrd on Unsplash.\n\n\n\nHow to cite\n\nKnipper, Alex, Naman Bansal, Jingyi Zheng, Wenying Li, and Shubhra Kanti Karmaker. 2023. “Food for Thought: First place winners – Auburn Big Data.” Real World Data Science, August 21, 2023. URL"
  },
  {
    "objectID": "case-studies/posts/2023/08/21/04-second-place-winners.html",
    "href": "case-studies/posts/2023/08/21/04-second-place-winners.html",
    "title": "Food for Thought: Second place winners – DeepFFTLink",
    "section": "",
    "text": "DeepFFTLink team members: Yang Wu and Kai Zhang are PhD students at Worcester Polytechnic Institute. Aishwarya Budhkar is a PhD student at Indiana University Bloomington. Xuhong Zhang is an assistant professor at Indiana University Bloomington. Xiaozhong Liu is an associate professor at Worcester Polytechnic Institute."
  },
  {
    "objectID": "case-studies/posts/2023/08/21/04-second-place-winners.html#perspective-on-the-challenge",
    "href": "case-studies/posts/2023/08/21/04-second-place-winners.html#perspective-on-the-challenge",
    "title": "Food for Thought: Second place winners – DeepFFTLink",
    "section": "Perspective on the challenge",
    "text": "Perspective on the challenge\nText matching is an essential task in natural language processing (NLP, Pang et al. 2016), while record linkage across different sources is an essential task in data science. Machine learning techniques allow people to combine data faster and cheaper than using manual linkage. However, in the context of the Food for Thought challenge, existing methods for matching universal product codes (UPCs) to ensemble codes (ECs) require every UPC to be compared with every EC code (Figure 1a). Such approaches can be computationally expensive in the training process when data is noisy. Here, we propose an ensemble model with a category-based adapter to tackle this problem, drawing on the category information included in UPC and EC data. The category-based adapter allows UPCs to be first matched with only a small and reliable set of ECs (Figure 1b). Then, an ensemble model will be deployed to make predictions for UPC-EC matching. Our proposed approach can achieve competitive performance compared with state-of-the-art models.\n\n\n\n\n\n(a)\n\n\n\n\n\n(b)\n\n\n\n\n\nFigure 1: A toy example of our method. Panel (a) shows the traditional matching method, while (b) is our proposed ensemble model with category-based adapter. With the help of the adapter, UPC 1 only needs to be matched with EC 1 and EC 3."
  },
  {
    "objectID": "case-studies/posts/2023/08/21/04-second-place-winners.html#our-approach",
    "href": "case-studies/posts/2023/08/21/04-second-place-winners.html#our-approach",
    "title": "Food for Thought: Second place winners – DeepFFTLink",
    "section": "Our approach",
    "text": "Our approach\nWe propose a two-step framework to address this problem. To begin with, we use a category-based adapter to get reliable candidate ECs for each UPC. Then, an ensemble model (Dietterich 2000) is deployed to make a prediction for each UPC-EC pair.\n\nCategory-based adapter\nBy using 2015–2016 UPC-EC data, we created a knowledge base, which is a UPC category–EC pair-wised table for generating candidate ECs. Within this setting, each UPC category is, on average, related to only 32 ECs. This knowledge base is then used as context to further filter the candidate ECs. Note that there are some new ECs generated year by year, which can also be part of the potential ECs in the UPC-EC matching task, since the contextual information of new ECs does not exist in our knowledge base.\n\n\nEnsembled model\nWe ensemble the base-string match and BERT models. BERT is a deep learning model for natural language processing (Devlin et al. 2018). In the base-string match model, we used the Term Frequency-Inverse Document Frequency (TFIDF) of each UPC and EC description as features to calculate a pairwise cosine similarity, which is a distance between instances. Meanwhile, we used features extracted from UPC and EC descriptions to fine-tune the BERT base model and calculated the cosine similarity of embeddings between each UPC and EC. Then we rank ECs based on their similarity scores with the UPC.\n\n\n\n\n\n\nFigure 2: The framework of our proposed model. A two-step strategy is used to make the final prediction.\n\n\n\n\n\n\n\nFind the code in the Real World Data Science GitHub repository."
  },
  {
    "objectID": "case-studies/posts/2023/08/21/04-second-place-winners.html#our-results",
    "href": "case-studies/posts/2023/08/21/04-second-place-winners.html#our-results",
    "title": "Food for Thought: Second place winners – DeepFFTLink",
    "section": "Our results",
    "text": "Our results\nWe randomly selected 500 samples from the 2017–2018 UPC-EC data to train the ensembled weight for each model. Two functions were adapted to make a fusion of base-string and BERT models:\n\\[\nC = a * X + b * Y  \n\\tag{1}\\]\n\\[\nC =  a * log(X) + b * log(Y) \\text{. }\n\\tag{2}\\]\n\\(C\\) denotes the final confidence score. \\(X\\) and \\(Y\\) represent base_string_similarity_score and BERT_similarity_score, respectively. \\(a\\) and \\(b\\) are corresponding model weights for base_string and BERT models.\nA better Success@5 is achieved with function (1). The ensembled weights for the base-string model and BERT model are 0.738 and 0.262, respectively. The experiment result indicates that the base_string model contributes more than the BERT model when the ensemble model makes predictions. The prediction result for the 2017–2018 data is:\n\nSuccess@5: 0.727\nNDCG@5: 0.528\n\nComputation time is 6 hours."
  },
  {
    "objectID": "case-studies/posts/2023/08/21/04-second-place-winners.html#future-work",
    "href": "case-studies/posts/2023/08/21/04-second-place-winners.html#future-work",
    "title": "Food for Thought: Second place winners – DeepFFTLink",
    "section": "Future work",
    "text": "Future work\nOur next step will focus on adding the newly generated EC data into our knowledge base, which allows the model to be more stable to make predictions for UPC-EC matching. Our model is an unsupervised method, which does not need labels for each instance. We use cosine similarity to rank the matches, so no labels are needed in the training process. However, our future work will try to label some instances to handle the UPC-EC matching task in a supervised manner."
  },
  {
    "objectID": "case-studies/posts/2023/08/21/04-second-place-winners.html#lessons-learned",
    "href": "case-studies/posts/2023/08/21/04-second-place-winners.html#lessons-learned",
    "title": "Food for Thought: Second place winners – DeepFFTLink",
    "section": "Lessons learned",
    "text": "Lessons learned\n\nIf the data is not complex, simple models may outperform complex models. For example, in our experiment, we found that the base-string model outperforms single RoBERTa (Liu et al. 2019) or BERT models. However, our ensemble model can outperform each individual model since model fusion allows information aggregation from multiple models.\nMulti-label models may not work well on UPC-EC data. In our early work, we tried to consider the UPC-EC matching task as a multi-label problem, e.g., we labeled each EC as a binary label which indicated whether the EC was an appropriate match or not. We mapped UPC and EC pairs into a multi-label table. However, we find that the UPC and EC keeps a one-to-one relation for most UPCs. The model performance of a multi-label model, i.e., Label-Specific Attention Network (LSAN, Xiao et al. 2019), is lower than base-string model on both Success@5 and NDCG@5 metrics.\n\n\n\n\n\n← Part 3: First place winners\n\n\n\n\nPart 5: Third place winners →\n\n\n\n\n\n\n\n\nAbout the authors\n\nYang Wu and Kai Zhang are PhD students, and Xiaozhong Liu is an associate professor at Worcester Polytechnic Institute. Aishwarya Budhkar is a PhD student and Xuhong Zhang is an assistant professor at Indiana University Bloomington.\n\n\n\n\n\nCopyright and licence\n\n© 2023 Yang Wu, Aishwarya Budhkar, Kai Zhang, Xuhong Zhang, and Xiaozhong Liu\n\n\n  This article is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0)  International licence. Thumbnail photo by Hanson Lu on Unsplash.\n\n\n\nHow to cite\n\nWu, Yang, Aishwarya Budhkar, Kai Zhang, Xuhong Zhang, and Xiaozhong Liu. 2023. “Food for Thought: Second place winners – DeepFFTLink.” Real World Data Science, August 21, 2023. URL"
  },
  {
    "objectID": "case-studies/posts/2023/08/21/05-third-place-winners.html",
    "href": "case-studies/posts/2023/08/21/05-third-place-winners.html",
    "title": "Food for Thought: Third place winners – Loyola Marymount",
    "section": "",
    "text": "Undergraduate student Yifan (Rosetta) Hu was responsible for writing the Python script that pre-processes the 2015–2016 UPC, EC, and PPC data for training neural network models. Her script randomly sampled five negative EC descriptions for every positive match between a UPC and EC code. Professor Mandy Korpusik performed the remaining work, including setting up the environment, training the BERT model, and evaluation. Hu spent roughly 10 hours on the competition, and Korpusik spent roughly 40 hours of work (and many additional hours running and monitoring the training and testing scripts)."
  },
  {
    "objectID": "case-studies/posts/2023/08/21/05-third-place-winners.html#our-perspective-on-the-challenge",
    "href": "case-studies/posts/2023/08/21/05-third-place-winners.html#our-perspective-on-the-challenge",
    "title": "Food for Thought: Third place winners – Loyola Marymount",
    "section": "Our perspective on the challenge",
    "text": "Our perspective on the challenge\nThe goal of this challenge is to use machine learning and natural language processing (NLP) to link language-based entries in the IRI and FNDDS databases. Our proposed approach is based on our prior work using deep learning models to map users’ natural language meal descriptions to the FNDDS database (Korpusik, Collins, and Glass 2017b) to retrieve nutrition information in a spoken diet tracking system. In the past, we found a trade-off between accuracy and cost, leading us to select convolutional neural networks over recurrent long short-term memory (LSTM) networks – with nearly 10x as many parameters and 2x the training time required, LSTMs achieved slightly lower performance on semantic tagging and food database mapping on meals in the breakfast category. Here, we propose to investigate state-of-the-art transformers, specifically the contextual embedding model (i.e., the entire sentence is used as context to generate the embedding) known as BERT (Bidirectional Encoder Representations from Transformers, Devlin et al. 2018).\n\nRelated work\nWithin the past few years, several papers have come out that learn contextual representations of sentences, where the entire sentence is used to generate embeddings.\nELMo (Peters et al. 2018) uses a linear combination of vectors extracted from intermediate layer representations of a bidirectional LSTM trained on a large text corpus as a language model; in this feature-based approach, the ELMo vector of the full input sentence is concatenated with the standard context-independent token representations and passed through a task-dependent model for final prediction. This showed performance improvement over state-of-the-art on six NLP tasks, including question answering, textual entailment, and sentiment analysis.\nOpenAI GPT (Radford et al. 2018) is a fine-tuning approach, where they first pre-train a multi-layer transformer (Vaswani et al. 2017) as a language model on a large text corpus, and then conduct supervised fine-tuning on the specific task of interest, with a linear softmax layer on top of the pre-trained transformer.\nGoogle’s BERT (2018) is a fine-tuning approach similar to GPT, but with the key difference that instead of combining separately trained forward and backward transformers, they instead use a masked language model for pre-training, where they randomly masked out input tokens and predicted only those tokens. They demonstrated state-of-the-art performance on 11 NLP tasks, including the CoNLL 2003 named entity recognition task, which is similar to our semantic tagging task.\nFinally, many models have recently been developed that improve upon BERT, including RoBERTa (which improves BERT’s pre-training by using bigger batches and more data, Y. Liu et al. 2019), XLNet (which uses Transformer-XL and avoids BERT’s pretrain-finetune discrepancy through learning a truly bidirectional context via permutations over the factorization order, Yang et al. 2019), and ALBERT (a lightweight BERT, Lan et al. 2019).\nIn our prior work on language understanding for nutrition (Korpusik et al. 2014, 2016; Korpusik and Glass 2017, 2018, 2019; Korpusik, Collins, and Glass 2017a), we used a similar binary classification approach for learning embeddings, which were then used at test time to map from user-described meals to USDA food database matches, but with convolutional neural networks (CNNs) instead of BERT. (BERT was not created until 2018, and due to limited memory available for deployment, we needed a smaller model than even BERT base, which has 100 million parameters.) Further work demonstrated that BERT outperformed CNNs on several language understanding tasks, including nutrition (Korpusik, Liu, and Glass 2019)."
  },
  {
    "objectID": "case-studies/posts/2023/08/21/05-third-place-winners.html#our-approach",
    "href": "case-studies/posts/2023/08/21/05-third-place-winners.html#our-approach",
    "title": "Food for Thought: Third place winners – Loyola Marymount",
    "section": "Our approach",
    "text": "Our approach\nOur approach is to fine-tune a large pre-trained BERT language model on the food data. BERT was originally trained on a massive amount of text for a language modelling task (i.e., predicting which word should come next in a sentence). It relies on a transformer model, which uses an “attention” mechanism to identify which words the model should pay the most “attention” to. We are specifically using BERT for binary sequence classification, which refers to predicting a label (i.e., classification) for a sequence of words. In our case, during fine-tuning (i.e., training the model further on our own dataset) we will feed the model pairs of sentences (where one sentence is the UPC description of a food item and the other is the EC description of another food item), and the model will perform binary classification, predicting whether the sentences are a match (i.e., 1) or not (i.e., 0). We start with the 2015–2016 ground truth PPC data for positive examples, and five randomly sampled negative examples per positive example.\n\nTraining methods\nSince we used a neural network model, the only features passed into our model were the tokenized words themselves of the EC and UPC food descriptions – we did not conduct any manual feature engineering (Dong and Liu 2018). The model was trained on a 90/10 split into 90% training and 10% validation data, where the validation data was used as a test set to fine-tune the model’s hyperparameters. We started with a randomly sampled set of 16,000 pairs, batch size of 16 (i.e., the model would train on batches of 16 samples at a time), AdamW (Loshchilov and Hutter 2017) as the optimizer (which adaptively updates the learning rate, or how large the update should be to the model’s parameters), a linear schedule with warmup (i.e., starting with a small learning rate in the first few epochs of training due to large variance in early stages of training, L. Liu et al. 2019), and one epoch (i.e., the number of times the model passes through all the training data). We then added the next randomly sampled set of 16,000 pairs to get a model trained on 32,000 data points. Finally, we reached a total of 48,000 data samples used for training. Each pair of sequences was tokenized with the pre-trained BERT tokenizer, with the special CLS and SEP tokens (where CLS is a learned vector that is typically passed to downstream layers for final classification, and SEP is a learned vector that separates two input sequences), and was padded with zeros to the maximum length input sequence of 240 tokens, so that each input sequence would be the same length.\n\n\nModel development approach\nWe faced many challenges due to the secure nature of the ADRF environment. Since our approach relies on BERT, we were blocked by errors due to the local BERT installation. Typically, BERT is downloaded from the web as the program runs. However, for this challenge, BERT must be installed locally for security reasons. To fix the errors, the BERT models needed to be installed with git lfs clone instead of git.\nSecond, we were unable to retrieve the test data from the database due to SQLAlchemy errors. We found a workaround by using DBeaver directly to save database tables as Excel spreadsheets, rather than accessing the database tables through Python.\nFinally, we needed a GPU in order to efficiently train our BERT models. However, we initially only had a CPU, so there was a delay due to setting up the GPU configuration. Once the GPU image was set up, there was still a CUDA error when running the BERT model during training. We determined that the model was too big to fit into GPU memory, so we found a workaround using gradient checkpointing (trading off computation speed for memory) with the transformers library’s Trainer and TrainingArguments. Unfortunately, the version of transformers we were using did not have these tools, and the library was not updated until less than a week before the deadline, so we still had to train the model on the CPU.\nTo deal with the inability to run jobs in the background, our process was checkpointing our models every five batches, and saving the model predictions during evaluation to a csv file every five batches as well.\n\n\n\n\n\n\nFind the code in the Real World Data Science GitHub repository."
  },
  {
    "objectID": "case-studies/posts/2023/08/21/05-third-place-winners.html#our-results",
    "href": "case-studies/posts/2023/08/21/05-third-place-winners.html#our-results",
    "title": "Food for Thought: Third place winners – Loyola Marymount",
    "section": "Our results",
    "text": "Our results\nAfter training, the 48K model (so-called because it was trained on 48,000 data samples) was used at test time via ranking all possible 2017–18 EC descriptions given an unseen UPC description. The rankings were obtained through the model’s output value – the higher the output (or confidence), the more highly we ranked that EC description. To speed up the ranking process, we used blocking (i.e., only ranking a subset of all possible matches), specifically with exact word matches (using only the first six words in the UPC description, which appeared to be the most important), and fed all possible matches through the model in one batch per UPC description. Since we still did not have sufficient time to complete evaluation on the full set of test UPC descriptions, we implemented an expedited evaluation that only considered the first 10 matching EC descriptions in the BERT ranking process (which we call BERT-FAST). We also report results for the slower evaluation method that considers all EC descriptions that match at least one of the first six words in a given UPC description, but note that these results are based on just a small subset of the total test set. See Table 1 below for our results, where the (5?) indicates how often the correct match was ranked among the top-5. See Table 2 for an estimate of how long it takes to train and test the model on a CPU.\n\n\n\n\nTable 1: S@5 and NCDG@5 for BERT, both for fast evaluation over the whole test set, and slower evaluation on a smaller subset (711 UPCs out of 37,693 total).\n\n\n\n\nModel\nSuccess@5\nNDCG@5\n\n\n\n\nBERT-FAST\n0.057\n0.047\n\n\nBERT-SLOW\n0.537\n0.412\n\n\n\n\n\n\nTable 2: An estimate of the time required to train and test the model.\n\n\n\n\n\nTime\n\n\n\n\nTraining (on 48K samples)\n16 hours\n\n\nTesting (BERT-FAST)\n52 hours\n\n\nTesting (BERT-SLOW)\n63 days"
  },
  {
    "objectID": "case-studies/posts/2023/08/21/05-third-place-winners.html#future-workrefinement",
    "href": "case-studies/posts/2023/08/21/05-third-place-winners.html#future-workrefinement",
    "title": "Food for Thought: Third place winners – Loyola Marymount",
    "section": "Future work/refinement",
    "text": "Future work/refinement\nIn the future, with more time available, we would train on all data, not just our limited dataset of 48,000 pairs, as well as perform evaluation on the held-out test set with the full set of possible EC matches that have one or more words in common with the UPC description. We would compare against baseline word embedding methods such as word2vec (Mikolov et al. 2017) and Glove (Pennington, Socher, and Manning 2014), and we would explore hierarchical prediction methods for improving efficiency and accuracy. Specifically, we would first train a classifier to predict the generic food category, and then train finer-grained models to predict specific foods within a general food category. Finally, we are exploring multi-modal transformer-based approaches that allow two input modalities (i.e., food images and text descriptions of a meal) for predicting the best UPC match."
  },
  {
    "objectID": "case-studies/posts/2023/08/21/05-third-place-winners.html#lessons-learned",
    "href": "case-studies/posts/2023/08/21/05-third-place-winners.html#lessons-learned",
    "title": "Food for Thought: Third place winners – Loyola Marymount",
    "section": "Lessons learned",
    "text": "Lessons learned\nWe recommend that future challenges provide every team with both a CPU and a GPU in their workspace, to avoid transitioning from one to the other midway through the challenge. In addition, if possible, it would be very helpful to provide a mechanism for running jobs in the background. Finally, it may be useful for teams to submit snippets of code along with library package names, in order for the installations to be tested properly beforehand.\n\n\n\n\n← Part 4: Second place winners\n\n\n\n\nPart 6: The value of competitions →\n\n\n\n\n\n\n\n\nAbout the authors\n\nYifan (Rosetta) Hu is an undergraduate student and Mandy Korpusik is an assistant professor of computer science at Loyola Marymount University’s Seaver College of Science and Engineering.\n\n\n\n\n\nCopyright and licence\n\n© 2023 Yifan Hu and Mandy Korpusik\n\n\n  This article is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0)  International licence. Thumbnail photo by Peter Bond on Unsplash.\n\n\n\nHow to cite\n\nHu, Yifan, and Mandy Korpusik. 2023. “Food for Thought: Third place winners – Loyola Marymount.” Real World Data Science, August 21, 2023. URL"
  },
  {
    "objectID": "case-studies/posts/2023/08/21/06-value-of-competitions.html",
    "href": "case-studies/posts/2023/08/21/06-value-of-competitions.html",
    "title": "Food for Thought: The value of competitions for confidential data",
    "section": "",
    "text": "We are witnessing a sea change in data collection practices by both governments and businesses – from purposeful collection (through surveys and censuses, for example) to opportunistic (drawing on web and social media data, and administrative datasets). This shift has made clear the importance of record linkage – a government might, for example, look to link records held by its various departments to understand how citizens make use of the gamut of public services.\nHowever, creating manual linkages between datasets can be prohibitively expensive, time consuming, and subject to human constraints and bias. Machine learning (ML) techniques offer the potential to combine data better, faster, and more cheaply. But, as the recently released National AI Research Resources Task Force report highlights, it is important to have an open and transparent approach to ensure that unintended biases do not occur.\nIn other words, ML tools are not a substitute for thoughtful analysis. Both private and public producers of a linked dataset have to determine the level of linkage quality – such as what precision/recall tradeoff is best for the intended purpose (that is, the balance between false-positive links and failure to cover links that should be there), how much processing time and cost is acceptable, and how to address coverage issues. The challenge is made more difficult by the idiosyncrasies of heterogeneous datasets, and more difficult yet when datasets to be linked include confidential data (Christensen and Miguel 2018; Christen, Ranbaduge, and Schnell 2020).\nAnd, of course, an ML solution is never the end of the road: many data linkage scenarios are highly dynamic, involving use cases, datasets, and technical ecosystems that change and evolve over time; effective use of ML in practice necessitates an ongoing and continuous investment (Koch et al. 2021). Because techniques are constantly improving, producers need to keep abreast of new approaches. A model that is working well today may no longer work in a year because of changes in the data, or because the organizational needs have changed so that a certain type of error is no longer acceptable. As Sculley et al. point out, “it is remarkably easy to incur massive ongoing maintenance costs at the system level when applying machine learning” (Sculley et al. 2014).\nAlso important is that record linkage is not seen as a technical problem relegated to the realm of computer scientists to solve. The full engagement of domain experts in designing the optimization problem, identifying measures of success, and evaluating the quality of the results is absolutely critical, as is building an understanding of the pros and cons of different measures (Schafer et al. 2021; Hand and Christen 2018). There will need to be much learning by doing in “sandbox” environments, and back and forth communication across communities to achieve successful outcomes, as noted in the recommendations of the Advisory Committee on Data for Evidence Building (a screenshot of which is shown in Figure 1).\n\n\n\n\n\n\nFigure 1: A recommendation for building an “innovation sandbox” as part of the creation of a new National Secure Data Service in the United States.\n\nDespite the importance of trial and error and transparency about linkage quality, there is no handbook that guides domain experts in how to design such sandboxes. There is a very real need for agreed-upon, domain-independent guidelines, or better yet, official standards to evaluate sandboxes. Those standards would define “who” could and would conduct the evaluation, and help guarantee independence and repeatability. And while innovation challenges have been embraced by the federal government, the devil can be very much in the details (Williams 2012).\nIt is for this reason that the approach taken in the Food for Thought linkage competition, and described in this compendium, provides an important first step towards a well specified, replicable framework for achieving high quality outcomes. In that respect it joins other recent efforts to bring together community-level research on shared sensitive data (MacAvaney et al. 2021; Tsakalidis et al. 2022). This competition, like those, helped bring to the foreground both the opportunities and challenges of doing research in secure sandboxes with sensitive data. Notably, these exercises highlight a kind of cultural tension between secure, managed environments, on the one hand, and unfettered machine learning research, on the other. The need for flexibility and agility in computational research bumps up against the need for advance planning and careful step-by-step processes in environments with well-defined data governance rules, and one of the key lessons learned is that the tradeoffs here need to be recognized and planned for.\nThis particular competition was important for a number of other reasons. Thanks to its organization as a competition, complete with prizes and bragging rights for strongly performing teams, it attracted new eyes from computer science and data science to think about how to address a critical real-world linkage problem. It offered the potential to produce approaches that were scalable, transparent, and reproducible. The engagement of domain experts and statisticians meant that it will be possible to conduct an informed error analysis, to explicitly relate the performance metrics in the task to the problem being solved in the real world, and to bring in the expertise of survey methodologists to think about the possible adjustments. And because it identified different approaches of addressing the same problem, it created an environment for new innovative ideas.\nMore generally, in addition to the excitement of the new approaches, this exercise laid bare the fragility of linkages in general and highlighted the importance of secure sandboxes for confidential data. While the promise of privacy preserving technologies is alluring as an alternative to bringing confidential data together in one place, such approaches are likely too immature to deploy ad hoc until a better understanding is established of how to translate real-world problems and their associated data into well-defined tasks, how to measure quality, and particularly how to assess the impact of match quality on different subgroups (Domingo-Ferrer, Sánchez, and Blanco-Justicia 2021). The scientific profession has gone through too painful a lesson with the premature application of differential privacy techniques to ignore the lessons that can be learned from a careful and systematic analysis of different approaches (2021; Van Riper et al. 2020; Ruggles et al. 2019; Giles et al. 2022).\nWe hope that the articles in this collection provide not only the first steps towards a handbook of best practices, but also an inspiration to share lessons learned, so that success can be emulated, and failures understood and avoided.\n\n\n\n\n← Part 5: Third place winners\n\n\n\n\nFind more case studies\n\n\n\n\n\n\n\n\nAbout the authors\n\nSteven Bedrick is an associate professor in Oregon Health and Science University’s Department of Medical Informatics and Clinical Epidemiology.\n\n\nOphir Frieder is a professor in Georgetown University’s Department of Computer Science, and in the Department of Biostatistics, Bioinformatics & Biomathematics at Georgetown University Medical Center.\n\n\nJulia Lane is a professor at the NYU Wagner Graduate School of Public Service and a NYU Provostial Fellow for Innovation Analytics. She co-founded the Coleridge Initiative.\n\n\nPhilip Resnik holds a joint appointment as professor in the University of Maryland Institute for Advanced Computer Studies and the Department of Linguistics, and an affiliate professor appointment in computer science.\n\n\n\nCopyright and licence\n\n© 2023 Steven Bedrick, Ophir Frieder, Julia Lane, and Philip Resnik\n\n\n  This article is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0)  International licence. Thumbnail photo by Alexandru Tugui on Unsplash.\n\n\n\nHow to cite\n\nBedrick, Steven, Ophir Frieder, Julia Lane, and Philip Resnik. 2023. “Food for Thought: The value of competitions for confidential data.” Real World Data Science, August 21, 2023. URL\n\n\n\n\n\n\n\n\n\nReferences\n\nChristen, P., T. Ranbaduge, and R. Schnell. 2020. Linking Sensitive Data - Methods and Techniques for Practical Privacy-Preserving Information Sharing. Springer. https://doi.org/10.1007/978-3-030-59706-1.\n\n\nChristensen, G., and E. Miguel. 2018. “Transparency, Reproducibility, and the Credibility of Economics Research.” Journal of Economic Literature 56 (3): 920–80. https://doi.org/10.1257/jel.20171350.\n\n\nDomingo-Ferrer, J., D. Sánchez, and A. Blanco-Justicia. 2021. “The Limits of Differential Privacy (and Its Misuse in Data Release and Machine Learning).” Communications of the ACM 64 (7): 33–35. https://doi.org/10.1145/3433638.\n\n\nGiles, O., K. Hosseini, G. Mingas, O. Strickson, L. Bowler, C. Rangel Smith, H. Wilde, et al. 2022. “Faking Feature Importance: A Cautionary Tale on the Use of Differentially-Private Synthetic Data.” https://arxiv.org/abs/2203.01363.\n\n\nHand, D., and P. Christen. 2018. “A Note on Using the f-Measure for Evaluating Record Linkage Algorithms.” Statistics and Computing 28 (3): 539–47. https://doi.org/10.1007/s11222-017-9746-6.\n\n\nKoch, B., E. Denton, A. Hanna, and J. G. Foster. 2021. “Reduced, Reused and Recycled: The Life of a Dataset in Machine Learning Research.” CoRR abs/2112.01716. https://arxiv.org/abs/2112.01716.\n\n\nMacAvaney, S., A. Mittu, G. Coppersmith, J. Leintz, and P. Resnik. 2021. “Community-Level Research on Suicidality Prediction in a Secure Environment: Overview of the CLPsych 2021 Shared Task.” In Proceedings of the Seventh Workshop on Computational Linguistics and Clinical Psychology: Improving Access, 70–80. Online: Association for Computational Linguistics. https://doi.org/10.18653/v1/2021.clpsych-1.7.\n\n\nRuggles, S., C. Fitch, D. Magnuson, and J. Schroeder. 2019. “Differential Privacy and Census Data: Implications for Social and Economic Research.” AEA Papers and Proceedings 109 (May): 403–8. https://doi.org/10.1257/pandp.20191107.\n\n\nSchafer, K. M., G. Kennedy, A. Gallyer, and P. Resnik. 2021. “A Direct Comparison of Theory-Driven and Machine Learning Prediction of Suicide: A Meta-Analysis.” PLOS ONE 16 (4): 1–23. https://doi.org/10.1371/journal.pone.0249833.\n\n\nSculley, D., G. Holt, D. Golovin, E. Davydov, T. Phillips, D. Ebner, V. Chaudhary, and M. Young. 2014. “Machine Learning: The High Interest Credit Card of Technical Debt.” In SE4ML: Software Engineering for Machine Learning (NIPS 2014 Workshop).\n\n\nTsakalidis, A., J. Chim, I. M. Bilal, A. Zirikly, D. Atzil-Slonim, F. Nanni, P. Resnik, et al. 2022. “Overview of the CLPsych 2022 Shared Task: Capturing Moments of Change in Longitudinal User Posts.” In Proceedings of the Eighth Workshop on Computational Linguistics and Clinical Psychology, 184–98. Seattle, USA: Association for Computational Linguistics. https://doi.org/10.18653/v1/2022.clpsych-1.16.\n\n\nVan Riper, D., T. Kugler, J. Schroeder, and S. Ruggles. 2020. “Differential Privacy and Racial Residential Segregation.” In 2020 APPAM Fall Research Conference.\n\n\nWilliams, H. 2012. “Innovation Inducement Prizes: Connecting Research to Policy.” Journal of Policy Analysis and Management 31 (3): 752–76. http://www.jstor.org/stable/41653827."
  },
  {
    "objectID": "contact.html",
    "href": "contact.html",
    "title": "Contact us",
    "section": "",
    "text": "Email: b.tarran@rss.org.uk\n\nTwitter: @rwdatasci\nLinkedIn: 4omics"
  },
  {
    "objectID": "contact.html#advertising-and-commercial",
    "href": "contact.html#advertising-and-commercial",
    "title": "Contact us",
    "section": "Advertising and commercial",
    "text": "Advertising and commercial\nEmail: advertising@rss.org.uk"
  },
  {
    "objectID": "ts-and-cs.html",
    "href": "ts-and-cs.html",
    "title": "Terms and conditions",
    "section": "",
    "text": "Statements of fact and opinion published on this website are those of the respective authors and contributors and not necessarily those of Real World Data Science (RWDS), its editors, the Royal Statistical Society (RSS), or other partners and funders.\nRWDS has prepared the content of this website responsibly and carefully. However, RWDS, its editors, the RSS, or other partners and funders disclaim all warranties, express or implied, as to the accuracy of the information contained in any of the materials on this website or on other linked websites or on any subsequent links. This includes, but is not by way of limitation:\n\nany implied warranties of merchantability and fitness for a particular purpose.\nany liability for damage to your computer hardware, data, information, materials and business resulting from the information or the lack of information available.\nany errors, omissions, or inaccuracies in the information.\nany decision made or action taken or not taken in reliance upon the information.\n\nRWDS, its editors, the RSS, or other partners and funders make no warranty as to the content, accuracy, timeliness or completeness of the information or that the information may be relied upon for any reason and bear no responsibility for the accuracy, content or legality of any linked site or for that of any subsequent links. RWDS, its editors, the RSS, or other partners and funders make no warranty that the website service will be uninterrupted or error-free or that any defects can be corrected.\nRWDS, its editors, the RSS, or other partners and funders shall not be liable for any losses or damages (including without limitation consequential loss or damage) whatsoever from the use of, or reliance on, the information in this website, or from the use of the internet generally. Links to other websites or the publication of advertisements do not constitute an endorsement or an approval by RWDS, its editors, the RSS, or other partners and funders.\nThese disclaimers and exclusions shall be governed by and construed in accordance with the laws of England and Wales under the exclusive jurisdiction of the courts of England and Wales. Those who choose to access this site from outside the United Kingdom are responsible for compliance with local laws if and to the extent local laws are applicable.\nBy using this site, you agree to these terms and conditions of use."
  },
  {
    "objectID": "ts-and-cs.html#site-content",
    "href": "ts-and-cs.html#site-content",
    "title": "Terms and conditions",
    "section": "Site content",
    "text": "Site content\nThis site and the “Real World Data Science” and “RWDS” brands and logos are copyright © The Royal Statistical Society.\nCopyright and licence terms for published articles and any associated videos, images, or other material can be found at the end of each article page. We make every reasonable effort to locate, contact and acknowledge copyright owners and wish to be informed by any copyright owners who are not properly identified and acknowledged on this website so that we may make any necessary corrections.\nWhere licence terms for individual articles, videos, images and other published content permit republication, you may do so in accordance with the stated terms of the respective licence(s).\nYou are not permitted to republish this site in its entirety."
  },
  {
    "objectID": "ts-and-cs.html#what-websites-do-we-link-to",
    "href": "ts-and-cs.html#what-websites-do-we-link-to",
    "title": "Terms and conditions",
    "section": "What websites do we link to?",
    "text": "What websites do we link to?\nRWDS editors and contributors recommend external web links on the basis of their suitability and usefulness for our users. Selection and addition of links to our website is entirely a matter for RWDS and for RWDS alone.\nIt is not our policy to enter into agreements for reciprocal links.\nThe inclusion of a link to an organisation’s or individual’s website does not constitute an endorsement or an approval by RWDS, its editors, the RSS, or other partners and funders of any product, service, policy or opinion of the organisation or individual. RWDS, its editors, the RSS, or other partners and funders are not responsible for the content of external websites."
  },
  {
    "objectID": "ts-and-cs.html#what-websites-will-we-not-link-to",
    "href": "ts-and-cs.html#what-websites-will-we-not-link-to",
    "title": "Terms and conditions",
    "section": "What websites will we not link to?",
    "text": "What websites will we not link to?\nWe will not link to websites that contain racist, sexual or misleading content; that promote violence; that are in breach of any UK law; which are otherwise offensive to individuals or to groups of people.\nThe decision of RWDS is final and no correspondence will be entered into.\nIf you wish to report a concern, please email b.tarran@rss.org.uk."
  },
  {
    "objectID": "ts-and-cs.html#software-and-services",
    "href": "ts-and-cs.html#software-and-services",
    "title": "Terms and conditions",
    "section": "Software and services",
    "text": "Software and services\nSource code and files for this site are available from GitHub. Use of our GitHub repository is governed by the Contributor Covenant Code of Conduct.\nThis site is built using Quarto, an open-source scientific and technical publishing system developed by Posit. Quarto source code and software licences are available from GitHub.\nReal World Data Science is hosted by GitHub Pages.\nThis site uses Google Analytics 4 for web analytics reporting.\nUser comments and reaction functionality is provided by giscus, a comments system powered by GitHub Discussions. Use of this comment functionality is governed by the Contributor Covenant Code of Conduct."
  },
  {
    "objectID": "ts-and-cs.html#notice-and-takedown-policy",
    "href": "ts-and-cs.html#notice-and-takedown-policy",
    "title": "Terms and conditions",
    "section": "Notice and Takedown policy",
    "text": "Notice and Takedown policy\nIf you are a rights holder and are concerned that you have found material on our site for which you have not given permission, or is not covered by a limitation or exception in national law, please contact us in writing stating the following:\n\nYour contact details.\nThe full bibliographic details of the material.\nThe exact and full url where you found the material.\nProof that you are the rights holder and a statement that, under penalty of perjury, you are the rights holder or are an authorised representative.\n\nContact details:\nNotice and Takedown,\nLicensing,\n12 Errol Street,\nLondon EC1Y 8LX\nweb@rss.org.uk\nUpon receipt of notification, the ‘Notice and Takedown’ procedure is then invoked as follows:\n\nWe will acknowledge receipt of your complaint by email or letter and will make an initial assessment of the validity and plausibility of the complaint.\nUpon receipt of a valid complaint the material will be temporarily removed from our website pending an agreed solution.\nWe will contact the contributor who deposited the material, if relevant. The contributor will be notified that the material is subject to a complaint, under what allegations, and will be encouraged to assuage the complaints concerned.\nThe complainant and the contributor will be encouraged to resolve the issue swiftly and amicably and to the satisfaction of both parties, with the following possible outcomes:\n\nThe material is replaced on our website unchanged.\nThe material is replaced on our website with changes.\nThe material is permanently removed from our website.\n\n\nIf the contributor and the complainant are unable to agree a solution, the material will remain unavailable through the website until a time when a resolution has been reached."
  },
  {
    "objectID": "ts-and-cs.html#contributor-covenant-code-of-conduct",
    "href": "ts-and-cs.html#contributor-covenant-code-of-conduct",
    "title": "Terms and conditions",
    "section": "Contributor Covenant Code of Conduct",
    "text": "Contributor Covenant Code of Conduct\n\nOur pledge\nWe as members, contributors, and leaders pledge to make participation in our community a harassment-free experience for everyone, regardless of age, body size, visible or invisible disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, caste, color, religion, or sexual identity and orientation.\nWe pledge to act and interact in ways that contribute to an open, welcoming, diverse, inclusive, and healthy community.\n\n\nOur standards\nExamples of behavior that contributes to a positive environment for our community include:\n\nDemonstrating empathy and kindness toward other people\nBeing respectful of differing opinions, viewpoints, and experiences\nGiving and gracefully accepting constructive feedback\nAccepting responsibility and apologizing to those affected by our mistakes, and learning from the experience\nFocusing on what is best not just for us as individuals, but for the overall community\n\nExamples of unacceptable behavior include:\n\nThe use of sexualized language or imagery, and sexual attention or advances of any kind\nTrolling, insulting or derogatory comments, and personal or political attacks\nPublic or private harassment\nPublishing others’ private information, such as a physical or email address, without their explicit permission\nOther conduct which could reasonably be considered inappropriate in a professional setting\n\n\n\nEnforcement responsibilities\nCommunity leaders are responsible for clarifying and enforcing our standards of acceptable behavior and will take appropriate and fair corrective action in response to any behavior that they deem inappropriate, threatening, offensive, or harmful.\nCommunity leaders have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, and will communicate reasons for moderation decisions when appropriate.\n\n\nScope\nThis Code of Conduct applies within all community spaces (encompassing this site, our GitHub repository, our social media channels, and any RWDS-organised online and offline events). It also applies when an individual is officially representing the community in public spaces. Examples of representing our community include using an official e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event.\nNote that unless prior permission is agreed in writing with the editor of RWDS, only the editor and editorial board of RWDS may officially represent the community. Comment to the media must only be given by appointed representatives and must be approved by the RSS press office.\n\n\nEnforcement\nInstances of abusive, harassing, or otherwise unacceptable behavior may be reported to the community leaders responsible for enforcement at b.tarran@rss.org.uk. All complaints will be reviewed and investigated promptly and fairly.\nAll community leaders are obligated to respect the privacy and security of the reporter of any incident.\n\n\nEnforcement guidelines\nCommunity leaders will follow these Community Impact Guidelines in determining the consequences for any action they deem in violation of this Code of Conduct:\n\n1. Correction\nCommunity Impact: Use of inappropriate language or other behavior deemed unprofessional or unwelcome in the community.\nConsequence: A private, written warning from community leaders, providing clarity around the nature of the violation and an explanation of why the behavior was inappropriate. A public apology may be requested.\n\n\n2. Warning\nCommunity Impact: A violation through a single incident or series of actions.\nConsequence: A warning with consequences for continued behavior. No interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, for a specified period of time. This includes avoiding interactions in community spaces as well as external channels like social media. Violating these terms may lead to a temporary or permanent ban.\n\n\n3. Temporary Ban\nCommunity Impact: A serious violation of community standards, including sustained inappropriate behavior.\nConsequence: A temporary ban from any sort of interaction or public communication with the community for a specified period of time. No public or private interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, is allowed during this period. Violating these terms may lead to a permanent ban.\n\n\n4. Permanent Ban\nCommunity Impact: Demonstrating a pattern of violation of community standards, including sustained inappropriate behavior, harassment of an individual, or aggression toward or disparagement of classes of individuals.\nConsequence: A permanent ban from any sort of public interaction within the community.\n\n\n\nAttribution\nThis Code of Conduct is adapted from the Contributor Covenant, version 2.1, available at https://www.contributor-covenant.org/version/2/1/code_of_conduct.html.\nCommunity Impact Guidelines were inspired by Mozilla’s code of conduct enforcement ladder.\nFor answers to common questions about this code of conduct, see the FAQ at https://www.contributor-covenant.org/faq. Translations are available at https://www.contributor-covenant.org/translations."
  }
]